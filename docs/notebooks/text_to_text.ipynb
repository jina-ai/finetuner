{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huf1E2zq7JWb"
      },
      "source": [
        "# Text-to-Text Search via BERT\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1Ui3Gw3ZL785I7AuzlHv3I0-jTvFFxJ4_?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "\n",
        "Searching large amounts of text documents with text queries is a very popular use-case and Finetuner enables you to accomplish this easily.\n",
        "\n",
        "This guide will lead you through an example use-case to show you how Finetuner can be used for text to text retrieval (Dense Retrieval).\n",
        "\n",
        "*Note, please consider switching to GPU/TPU Runtime for faster inference.*\n",
        "\n",
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSuWo72R7Sno"
      },
      "outputs": [],
      "source": [
        "!pip install 'finetuner[full]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPDhvWkw7kas"
      },
      "source": [
        "## Task\n",
        "\n",
        "In Finetuner, two BERT models are supported as backbones, namely `bert-base-cased` and `sentence-transformers/msmarco-distilbert-base-v3`, both of which are models hosted on Hugging Face.\n",
        "\n",
        "In this example, we will fine-tune `bert-base-cased` on the [Quora Question Pairs](https://www.sbert.net/examples/training/quora_duplicate_questions/README.html?highlight=quora#dataset) dataset, where the search task involves finding duplicate questions in the dataset. An example query for this search task might look as follows:\n",
        "\n",
        "```\n",
        "How can I be a good geologist?\n",
        "\n",
        "```\n",
        "\n",
        "Retrieved documents that could be duplicates for this question should then be ranked in the following order:\n",
        "\n",
        "```\n",
        "What should I do to be a great geologist?\n",
        "How do I become a geologist?\n",
        "What do geologists do?\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "We can fine-tune BERT so that questions that are duplicates of each other are represented in close proximity and questions that are not duplicates will have representations that are further apart in the embedding space. In this way, we can rank the embeddings in our search space by their proximity to the query question and return the highest ranking duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR6g0E_8fOz"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the [Quora Question Pairs](https://www.sbert.net/examples/training/quora_duplicate_questions/README.html?highlight=quora#dataset) dataset to show-case Finetuner for text to text search. We have already pre-processed this dataset and made it available for you to pull from Jina AI Cloud. Do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwS11Nsg7jPM"
      },
      "outputs": [],
      "source": [
        "import finetuner\n",
        "from docarray import DocumentArray, Document\n",
        "\n",
        "finetuner.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PIO5T--p4tR"
      },
      "outputs": [],
      "source": [
        "train_data = DocumentArray.pull('quora_train.da', show_progress=True)\n",
        "query_data = DocumentArray.pull('quora_query_dev.da', show_progress=True)\n",
        "index_data = DocumentArray.pull('quora_index_dev.da', show_progress=True)\n",
        "\n",
        "train_data.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_IlEIp59g9v"
      },
      "source": [
        "So we have 104598 training `Document`s. Each `Document` consists of a text field that contains the question, as well as a `finetuner_label` which indicates the label to which the question belongs. If multiple questions have the same label, they are duplicates of one another. If they have different `finetuner_label`s, they have no duplicates of each other.\n",
        "\n",
        "As for the evaluation dataset, we load `query_data` and `index_data` separately. The `query_data` have the same structure as the `train_data`, consisting of labeled documents. The `index_data` are the data against which the queries will be matched, and contain many documents, some of which may be irrelevant to the queries (i.e. they have no duplicated in the `query_data`).\n",
        "If you look at the summaries for the `query_data` and `index_data`, you will find that they have the following number of samples:\n",
        "\n",
        "```\n",
        "Length of queries DocumentArray: 5000\n",
        "Length of index DocumentArray: 15746\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYrABkN9vYO"
      },
      "source": [
        "## Backbone model\n",
        "To keep things simple, we have decided to fine-tune the BERT model `bert-base-cased`. We could also have chosen `sentence-transformers/msmarco-distilbert-base-v3` as our base model, which has already been fine-tuned on the MSMarco dataset. \n",
        "However, for the purpose of this experiment, we want to explore how much improvement in performance we can gain from fine-tuning `bert-base-cased` on the Quora Question Pairs dataset using Finetuner. \n",
        "Perhaps in the future, we might want to create another run where we experiment with fine-tuning other BERT models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAlQArUB99oG"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Now that we have the training and evaluation datasets loaded as `DocumentArray`s and selected our model, we can start our fine-tuning run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsRfjf1Z8ymZ"
      },
      "outputs": [],
      "source": [
        "from finetuner.callback import EvaluationCallback\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='bert-base-cased',\n",
        "    train_data='quora_train.da',\n",
        "    loss='TripletMarginLoss',\n",
        "    optimizer='Adam',\n",
        "    learning_rate = 1e-5,\n",
        "    epochs=3,\n",
        "    batch_size=128,\n",
        "    device='cuda',\n",
        "    callbacks=[\n",
        "        EvaluationCallback(\n",
        "            query_data='quora_query_dev.da',\n",
        "            index_data='quora_index_dev.da',\n",
        "            batch_size=32\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_MxAW9E-ddZ"
      },
      "source": [
        "Our fine-tuning call has a lot of arguments. Let's discuss what the most important ones are responsible for. \n",
        "\n",
        "Most importantly, we select our model with `model='bert-base-cased'` and pass our training data with `train_data=train_data`. These two arguments are required. \n",
        "We set our `experiment_name` to `'finetune-quora-dataset'` and our `run_name` to `'finetune-quora-dataset-bert-base-cased'`. \n",
        "This will make it easy for us to retrieve the experiment and run in the future. We also provide a short description of our run, just for some extra context. \n",
        "\n",
        "For this run, we select Finetuner's `TripletMarginLoss` and `TripletMarginMiner`, as they are most relevant for our use-case. The `TripletMarginLoss` measures the similarity between three tensors, namely the anchor, a positive sample and a negative sample. This makes sense for our task, since we want duplicate questions to have representations closer together, while non-duplicates should have more dissimilar representations. Likewise, the `TripletMarginMiner` outputs a tuple of size 3, with an anchor, a positive sample and a negative sample.\n",
        "\n",
        "Lastly, we provide an `EvaluationCallback` with our `query_data` and `index_data`. This evaluation is done at the end of each epoch and its results will be visible to us in the logs, which we will monitor in the next section. Since we have not specified which metrics should be applied, default metrics will be computed. The `Evaluation` section of this guide will show you the default metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0DGNRo8-lZD"
      },
      "source": [
        "## Monitoring\n",
        "\n",
        "Now that we've created a run, let's see its status. You can monitor the run by checking the status - `run.status()`, -and the logs `run.logs()` or `run.stream_logs()`. \n",
        "\n",
        "*note, the job will take around 15 minutes to finish.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gajka0TG-S6u"
      },
      "outputs": [],
      "source": [
        "for entry in run.stream_logs():\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AuB0IWC_CSt"
      },
      "source": [
        "Dependending on the size of the training data, some runs might take up to several hours. You can later reconnect to your run very easily to monitor its status.\n",
        "\n",
        "```python\n",
        "import finetuner\n",
        "\n",
        "finetuner.login()\n",
        "run = finetuner.get_run('finetune-quora-dataset-bert-base-cased')\n",
        "print(f'Run status: {run.status()}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agqrb0TX_Y4b"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "Our `EvaluationCallback` during fine-tuning ensures that after each epoch, an evaluation of our model is run. We can access the evaluation results in the logs as follows `print(run.logs())`:\n",
        "\n",
        "```bash\n",
        "  Training [3/3] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 818/818 0:00:00 0:03:05 • loss: 0.000\n",
        "[15:36:40] DEBUG    Metric: 'model_average_precision' Value: 0.95728                                     __main__.py:202\n",
        "           DEBUG    Metric: 'model_dcg_at_k' Value: 1.33912                                              __main__.py:202\n",
        "           DEBUG    Metric: 'model_f1_score_at_k' Value: 0.13469                                         __main__.py:202\n",
        "           DEBUG    Metric: 'model_hit_at_k' Value: 0.99720                                              __main__.py:202\n",
        "           DEBUG    Metric: 'model_ndcg_at_k' Value: 0.97529                                             __main__.py:202\n",
        "           DEBUG    Metric: 'model_precision_at_k' Value: 0.07653                                        __main__.py:202\n",
        "           DEBUG    Metric: 'model_r_precision' Value: 0.94393                                           __main__.py:202\n",
        "           DEBUG    Metric: 'model_recall_at_k' Value: 0.99301                                           __main__.py:202\n",
        "           DEBUG    Metric: 'model_reciprocal_rank' Value: 0.96686                                       __main__.py:202\n",
        "           INFO     Done ✨                                                                              __main__.py:204\n",
        "           INFO     Saving fine-tuned models ...                                                         __main__.py:207\n",
        "           INFO     Saving model 'model' in /usr/src/app/tuned-models/model ...                          __main__.py:218\n",
        "[15:36:41] INFO     Pushing saved model to Jina AI Cloud ...                                                    __main__.py:225\n",
        "[15:37:32] INFO     Pushed model artifact ID: '62b9cb73a411d7e08d18bd16'                                 __main__.py:231\n",
        "           INFO     Finished 🚀                                                                          __main__.py:233                                                  __main__.py:225\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTfBfB8A_1fO"
      },
      "source": [
        "## Saving\n",
        "Once your run has successfully completed, you can save your fine-tuned model in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7AJw3X9-7C-"
      },
      "outputs": [],
      "source": [
        "artifact = run.save_artifact('bert-model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYgPIR_kAI6z"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now you saved the `artifact` into your host machine,\n",
        "let's use the fine-tuned model to encode a new `Document`:\n",
        "\n",
        "```{admonition} Inference with ONNX\n",
        "In case you set `to_onnx=True` when calling `finetuner.fit` function,\n",
        "please use `model = finetuner.get_model(artifact, is_onnx=True)`\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs2G-rNFAJ4I"
      },
      "outputs": [],
      "source": [
        "model = finetuner.get_model(artifact=artifact, device='cuda')\n",
        "\n",
        "query = DocumentArray([Document(text='How can I be an engineer?')])\n",
        "\n",
        "finetuner.encode(model=model, data=query)\n",
        "finetuner.encode(model=model, data=index_data)\n",
        "assert query.embeddings.shape == (1, 768)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vUDidVIkh7"
      },
      "source": [
        "And finally you can use the embeded `query` to find top-k semantically related text within `index_data` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_bM-TXRE2h7"
      },
      "outputs": [],
      "source": [
        "query.match(index_data, limit=10, metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before and After\n",
        "We can directly compare the results of our fine-tuned model with a its zero-shot counterpart to getter a better idea of how finetuning affects the results of a search. While the differences between the two models may be subtle for some queries, the examples below show a clear improvement in the quality of the search results:\n",
        "\n",
        "```python\n",
        "import copy\n",
        "\n",
        "query_pt = DocumentArray.pull('quora_query_dev.da', show_progress=True)\n",
        "index_pt = DocumentArray.pull('quora_index_dev.da', show_progress=True)\n",
        "\n",
        "query_ft = copy.deepcopy(query_pt)\n",
        "index_ft = copy.deepcopy(index_pt)\n",
        "\n",
        "model_pt = finetuner.build_model('bert-base-cased')\n",
        "\n",
        "finetuner.encode(model=model, data=query_ft)\n",
        "finetuner.encode(model=model, data=index_ft)\n",
        "\n",
        "finetuner.encode(model=model_pt, data=query_pt)\n",
        "finetuner.encode(model=model_pt, data=index_pt)\n",
        "\n",
        "query_ft.match(index_ft)\n",
        "query_py.match(index_pt)\n",
        "\n",
        "examples = [7, 10]\n",
        "\n",
        "for i, (doc_pt, doc_ft) in enumerate(zip(query_pt, query_ft)):\n",
        "  if i in examples:\n",
        "    print(f'\\nQuery: {doc_ft.text}')\n",
        "    print(' matches pretrained:')\n",
        "    for match in doc_pt.matches[:5]:\n",
        "      print(f' - {match.text}')\n",
        "    print(' matches finetuned')\n",
        "    for match in doc_ft.matches[:5]:\n",
        "      print(f' - {match.text}')\n",
        "\n",
        "```\n",
        "\n",
        "```plaintext\n",
        "\n",
        "Query: What's the best way to start learning robotics?\n",
        " matches pretrained:\n",
        " - What is the best way to start with robotics?\n",
        " - What is the best way to learn web programming?\n",
        " - What is the best way to start learning Japanese from scratch?\n",
        " - What is the best way to start learning a language?\n",
        " - What is the best place to learn data science?\n",
        " matches finetuned\n",
        " - What is good way to learn robotics?\n",
        " - What is the best way to start with robotics?\n",
        " - How can I get started learning about robotics?\n",
        " - How can I start to learn robotics from zero?\n",
        " - From where should a complete beginner (0 knowledge) start in learning robotics?\n",
        "\n",
        "Query: What online platforms can I post ads for beer money opportunity?\n",
        " matches pretrained:\n",
        " - On what online platforms can I post ads for beer money opportunity?\n",
        " - How can I restore a mobile-number only Facebook messenger account?\n",
        " - How do I earn money online without any blog or website?\n",
        " - Do I need to register my self to selling products on online platforms in India?\n",
        " - Which is the best website where we can buy instagram followers and likes?\n",
        " matches finetuned\n",
        " - On what online platforms can I post ads for beer money opportunity?\n",
        " - What are some legit ways to earn money online?\n",
        " - What are some genuine ways to earn money online?\n",
        " - What are the best legitimate methods to making money online?\n",
        " - What are the legitimate ways to earn money online?\n",
        "\n",
        "```\n",
        "\n",
        "You can see that the pretrained model returned questions that were written similarly to the initial query, but about different topics. On the other hand, the finetuned model returned questions that were always relevant to the query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czK5pSUEAcdS"
      },
      "source": [
        "That's it!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.15 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "9ad9c14fbc5ce15e23594239b0b0bb7cf990b71472055d7d43822c20d61e1cff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
