{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Synthesis\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1sX5K0eophlHXu1S7joysZJUj1zfh28Gi?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "\n",
        "When using Finetuner, each item in your training data must either have a label, or have a similarity score comparing it to some other item. See the Finetuner documentation on [preparing training data](https://finetuner.jina.ai/walkthrough/create-training-data/).\n",
        "If your data is not labelled, and you don't want to spend time manually organizing and labelling it, you can use the `finetuner.synthesize` function to automatically construct a dataset that can be used in training.\n",
        "\n",
        "This guide will walk you through the process of using the `finetuner.synthesize` function, as well as how to use its output for training.\n",
        "\n",
        "![synthesis_flowchart](https://user-images.githubusercontent.com/58855099/240291609-5b3711d6-7c1b-4656-882e-5de9b488d395.png)\n",
        "\n",
        "\n",
        "### Install"
      ],
      "metadata": {
        "id": "RDjy9CrsuHH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2JbPtGVRVMo"
      },
      "outputs": [],
      "source": [
        "!pip install 'finetuner[full]'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Synthesis Data\n",
        "To perform synthesis, we need a query dataset and a corpus dataset, with the query dataset containing examples of user queries, and the corpus containing example search results.\n",
        "\n",
        "We'll be generating training data based on the electronics section of the [Amazon cross-market dataset](https://xmrec.github.io/data/us/), a collection of products, ratings and reviews taken from Amazon. For our purposes, we will only be using the product names.  \n",
        "\n",
        "We use the `xmarket_queries_da` and `xmarket_corpus_da` datasets, which we have already pre-processed and made available on the Jina AI Cloud. You can access them using `DocumentArray.pull`:"
      ],
      "metadata": {
        "id": "IRctQj4-zF9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import finetuner\n",
        "from docarray import Document, DocumentArray\n",
        "\n",
        "finetuner.login(force=True)"
      ],
      "metadata": {
        "id": "Srywu6C3YB0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_data = DocumentArray.pull('finetuner/xmarket_queries_da')\n",
        "corpus_data = DocumentArray.pull('finetuner/xmarket_corpus_da')\n",
        "\n",
        "query_data.summary()\n",
        "query_data[0].summary()"
      ],
      "metadata": {
        "id": "hupAvfrwXJFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of the data in these `DocumentArray`s is very simple, each `Document` wraps a single item, contained in its `text` field."
      ],
      "metadata": {
        "id": "Xv1Qz1Q3mYu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing models\n",
        "Data synthesis jobs require two different models: a relation miner and a cross encoder.  \n",
        "\n",
        "The relation miner is used to identify one similar and several dissimilar documents from the corpus data for each query in the query data.  \n",
        "\n",
        "The cross encoder is then used to calculate a similarity between each query and its corresponding (dis)similar documents.  \n",
        "\n",
        "Currently, we support synthesis jobs for data in English or Multilingual, so when choosing a model you can just provide the `synthesis_model_en` or `synthesis_model_multi` object which contains the appropriate models for each of these tasks."
      ],
      "metadata": {
        "id": "pLoVzibX6BB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Synthesis Run\n",
        "Now that we have the query and corpus datasets loaded as `DocumentArray`s, we can begin our synthesis run. We only need to provide the query and corpus data and the models that we are using.  \n",
        "\n",
        "The `num_relations` parameter is set to 10. This parameter determines how many documents are retrieved for each query. There will always be one similar document and `(num_relations - 1)` dissimilar documents retrieved. These dissimilar documents are what make up the generated documents, so the size of the generated `DocumentArray` is always equal to `len(query_data) * (num_relations - 1)`. By default this parameter is set to 10, meaning that the size of the generated dataset would be twice as large as the size of the query dataset."
      ],
      "metadata": {
        "id": "KXtNctnH50AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from finetuner.model import synthesis_model_en\n",
        "\n",
        "synthesis_run = finetuner.synthesize(\n",
        "    query_data='finetuner/xmarket_queries_da',\n",
        "    corpus_data='finetuner/xmarket_corpus_da',\n",
        "    models=synthesis_model_en,\n",
        "    num_relations=20,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7_EmudwyZlCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring\n",
        "\n",
        "Now that we've created a run, we can check its status. You can monitor the run's progress with the function `synthesis_run.status()`, and the logs with `synthesis_run.logs()` or `synthesis_run.stream_logs()`. \n",
        "\n",
        "*Note: The job will take around 15 minutes to finish.*"
      ],
      "metadata": {
        "id": "93yAUv4q-FQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in synthesis_run.stream_logs():\n",
        "  print(entry)"
      ],
      "metadata": {
        "id": "bZWaP1hbiA-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependending on the size of the training data, some runs might take up to several hours. You can easily reconnect to your run later to monitor its status.\n",
        "\n",
        "```python\n",
        "import finetuner\n",
        "\n",
        "finetuner.login()\n",
        "synthesis_run = finetuner.get_run('my-synthesis-run')\n",
        "print(f'Run status: {run.status()}')\n",
        "```"
      ],
      "metadata": {
        "id": "wZL1O-YK-8kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieving the data\n",
        "\n",
        "Once the synthesis run has finished, the synthesised data will be pushed to the Jina AI Cloud under your account. The name of the pushed `DocumentArray` will be stored in `synthesis_run.train_data`."
      ],
      "metadata": {
        "id": "DoOuKaDU_F8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_name = synthesis_run.train_data\n",
        "train_data = DocumentArray.pull(train_data_name)\n",
        "train_data.summary()"
      ],
      "metadata": {
        "id": "i6iiKEf7nyMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training with Synthesised Data\n",
        "\n",
        "Using your synthesised data, you can now train a model using the `MarginMSELoss` function.  \n",
        "\n",
        " We have prepared the index and query datasets `xmarket-gpl-eval-queries` and `xmarket-gpl-eval-queries` so that we can evaluate the improvement provided by training on this data:\n",
        "\n",
        " Note: if you use `synthesis_model_multi` for training data synthesis on languages other than English, please choose `distiluse-base-multi` or `bert-base-multi` as the backbone embedding model."
      ],
      "metadata": {
        "id": "cisFVD3o_bx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from finetuner.callback import EvaluationCallback\n",
        "\n",
        "training_run = finetuner.fit(\n",
        "    model='sbert-base-en',\n",
        "    train_data=synthesis_run.train_data,\n",
        "    loss='MarginMSELoss',\n",
        "    optimizer='Adam',\n",
        "    learning_rate=1e-5,\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        EvaluationCallback(\n",
        "            query_data='finetuner/xmarket-gpl-eval-queries',\n",
        "            index_data='finetuner/xmarket-gpl-eval-index',\n",
        "            batch_size=32,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ebfxt4NStvvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as before, you can monitor the progress of your run using `training_run.stream_logs()`:"
      ],
      "metadata": {
        "id": "ubApI8OxARz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in training_run.stream_logs():\n",
        "  print(entry)"
      ],
      "metadata": {
        "id": "5tXpHElN4zzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating\n",
        "\n",
        "Our `EvaluationCallback` during fine-tuning ensures that after each epoch, an evaluation of our model is run. We can access the evaluation results in the logs using `print(training_run.logs())`:\n",
        "\n",
        "```bash\n",
        "Training [3/3] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 470/470 0:00:00 0:02:34 • loss: 5.191\n",
        "INFO     Done ✨                                                                              __main__.py:192\n",
        "DEBUG    Finetuning took 0 days, 0 hours 11 minutes and 55 seconds                            __main__.py:194\n",
        "INFO     Metric: 'precision_at_k' before fine-tuning:  0.16063 after fine-tuning: 0.20824     __main__.py:207\n",
        "INFO     Metric: 'recall_at_k' before fine-tuning:  0.29884 after fine-tuning: 0.39044        __main__.py:207\n",
        "INFO     Metric: 'f1_score_at_k' before fine-tuning:  0.13671 after fine-tuning: 0.18335      __main__.py:207\n",
        "INFO     Metric: 'hit_at_k' before fine-tuning:  0.64277 after fine-tuning: 0.70012           __main__.py:207\n",
        "INFO     Metric: 'average_precision' before fine-tuning:  0.34342 after fine-tuning: 0.41825  __main__.py:207\n",
        "INFO     Metric: 'reciprocal_rank' before fine-tuning:  0.40001 after fine-tuning: 0.47258    __main__.py:207\n",
        "INFO     Metric: 'dcg_at_k' before fine-tuning:  1.49599 after fine-tuning: 1.89955           __main__.py:207\n",
        "fine-tuning:  1.49618 after fine-tuning: 1.77899\n",
        "INFO     Building the artifact ...                                                            __main__.py:231\n",
        "INFO     Pushing artifact to Jina AI Cloud ...                                                __main__.py:260\n",
        "```\n",
        "\n",
        "The amount of improvement is highly dependent on the amount of data generated during synthesis, **as the amount of training data increases, so will the performance of the finetuned model**. To increase the number of documents generated, we can either increase the size of the query dataset provided to the `finetuner.synthesize` function, or increase value of the `num_relations` parameter, which will result in more documents being generated per query. Conversely, choosing a smaller value for `num_relations` would result in shorter generation and training times, but less improvement after training.\n",
        "  \n",
        "To better understand the relationship between the amount of training data and the increase in performance, have a look at the [how much data?](https://finetuner.jina.ai/advanced-topics/budget/) section of our documentation.\n"
      ],
      "metadata": {
        "id": "UcB3Fyk5Ao6T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "521y_tPFXM6C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
