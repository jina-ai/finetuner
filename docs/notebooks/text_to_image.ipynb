{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UCyCMPcvLGw"
   },
   "source": [
    "# Text-to-Image Search via CLIP\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1yKnmy2Qotrh3OhgwWRsMWPFwOSAecBxg?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
    "\n",
    "Traditionally, searching images from text (text-image-retrieval) relies heavily on human annotations, this is commonly referred to as *Text/Tag based Image Retrieval (TBIR)*.\n",
    "\n",
    "The [OpenAI CLIP](https://github.com/openai/CLIP) model maps the dense vectors extracted from text and image into the same semantic space and produces a strong zero-shot model to measure the similarity between text and images.\n",
    "\n",
    "This guide will showcase fine-tuning a `CLIP` model for text to image retrieval.\n",
    "\n",
    "*Note, please consider switching to GPU/TPU Runtime for faster inference.*\n",
    "\n",
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vglobi-vvqCd"
   },
   "outputs": [],
   "source": [
    "!pip install 'finetuner[full]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXddluSIwCGW"
   },
   "source": [
    "## Task\n",
    "We'll be fine-tuning CLIP on the [fashion captioning dataset](https://github.com/xuewyang/Fashion_Captioning) which contains information about fashion products.\n",
    "\n",
    "For each product the dataset contains a title and images of multiple variants of the product. We constructed a parent [`Document`](https://docarray.jina.ai/fundamentals/document/#document) for each picture, which contains two [chunks](https://docarray.jina.ai/fundamentals/document/nested/#nested-structure): an image document and a text document holding the description of the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVBez7dHwIye"
   },
   "source": [
    "## Data\n",
    "Our journey starts locally. We have to [prepare the data and push it to the Jina AI Cloud](https://finetuner.jina.ai/walkthrough/create-training-data/) and Finetuner will be able to get the dataset by its name. For this example,\n",
    "we already prepared the data, and we'll provide the names of traning and evaluation data (`fashion-train-data-clip` and `fashion-eval-data-clip`) directly to Finetuner.\n",
    "In addition, we also provide labeled queries and an index of labeled documents for evaluating the retrieval capabilities of the resulting fine-tuned model stored in the datasets `fashion-eval-data-queries` and `fashion-eval-data-index`.\n",
    "\n",
    "\n",
    "```{admonition} Push data to the cloud\n",
    "We don't require you to push data to the Jina AI Cloud by yourself. Instead of a name, you can provide a `DocumentArray` and Finetuner will do the job for you.\n",
    "When working with documents where images are stored locally, please call `doc.load_uri_to_blob()` to reduce network transmission and speed up training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfPZBQVxxEHm"
   },
   "outputs": [],
   "source": [
    "import finetuner\n",
    "from docarray import DocumentArray, Document\n",
    "\n",
    "finetuner.login(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpIj7viExFti"
   },
   "outputs": [],
   "source": [
    "train_data = DocumentArray.pull('fashion-train-data-clip', show_progress=True)\n",
    "eval_data = DocumentArray.pull('fashion-eval-data-clip', show_progress=True)\n",
    "query_data = DocumentArray.pull('fashion-eval-data-queries', show_progress=True)\n",
    "index_data = DocumentArray.pull('fashion-eval-data-index', show_progress=True)\n",
    "\n",
    "train_data.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE87a5Nvwd7q"
   },
   "source": [
    "## Backbone model\n",
    "Currently, we support several CLIP variations from [open-clip](https://github.com/mlfoundations/open_clip) for text to image retrieval tasks.\n",
    "\n",
    "However, you can see all available models either in [choose backbone](https://finetuner.jina.ai/walkthrough/choose-backbone/) section or by calling `finetuner.describe_models()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81fh900Bxgkn"
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Now that we have the training and evaluation datasets loaded as `DocumentArray`s and selected our model, we can start our fine-tuning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDcpfybOv1dh"
   },
   "outputs": [],
   "source": [
    "from finetuner.callback import EvaluationCallback\n",
    "\n",
    "run = finetuner.fit(\n",
    "    model='openai/clip-vit-base-patch32',\n",
    "    train_data='fashion-train-data-clip',\n",
    "    eval_data='fashion-eval-data-clip',\n",
    "    epochs=5,\n",
    "    learning_rate= 1e-7,\n",
    "    loss='CLIPLoss',\n",
    "    device='cuda',\n",
    "    callbacks=[\n",
    "        EvaluationCallback(\n",
    "            model='clip-text',\n",
    "            index_model='clip-vision',\n",
    "            query_data='fashion-eval-data-queries',\n",
    "            index_data='fashion-eval-data-index',\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPDmFdubxzUE"
   },
   "source": [
    "Let's understand what this piece of code does:\n",
    "\n",
    "* We start with providing `model`, names of training and evaluation data.\n",
    "* We also provide some hyper-parameters such as number of `epochs` and a `learning_rate`.\n",
    "* We use `CLIPLoss` to optimize the CLIP model.\n",
    "* We use an evaluation callback, which uses the `'clip-text'` model for encoding the text queries and the `'clip-vision'` model for encoding the images in `'fashion-eval-data-index'`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKv3VcMKyG8d"
   },
   "source": [
    "## Monitoring\n",
    "\n",
    "Now that we've created a run, let's see its status. You can monitor the run by checking the status - `run.status()` and - the logs - `run.logs()` or - `run.stream_logs()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX45y-2fxs4L"
   },
   "outputs": [],
   "source": [
    "# note, the fine-tuning might takes 20~ minutes\n",
    "for entry in run.stream_logs():\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi49YlQsyXbi"
   },
   "source": [
    "Since some runs might take up to several hours/days, it's important to know how to reconnect to Finetuner and retrieve your run.\n",
    "\n",
    "```python\n",
    "import finetuner\n",
    "\n",
    "finetuner.login()\n",
    "run = finetuner.get_run(run.name)\n",
    "```\n",
    "\n",
    "You can continue monitoring the run by checking the status - `finetuner.run.Run.status()` or the logs - `finetuner.run.Run.logs()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xeq_aVRxyqlW"
   },
   "source": [
    "## Evaluating\n",
    "\n",
    "Our `EvaluationCallback` during fine-tuning ensures that after each epoch, an evaluation of our model is run. We can access the results of the last evaluation in the logs as follows `print(run.logs())`:\n",
    "\n",
    "```bash\n",
    "  Training [5/5] ━━━━ 195/195 0:00… 0:0… • loss: 2.419 • val_loss: 2.803\n",
    "[13:32:41] INFO     Done ✨                              __main__.py:195\n",
    "           DEBUG    Finetuning took 0 days, 0 hours 5 minutes and 30 seconds\n",
    "           DEBUG    Metric: 'clip-text-to-clip-vision_precision_at_k' Value: 0.28532                                                   \n",
    "           DEBUG    Metric: 'clip-text-to-clip-vision_hit_at_k' Value: 0.94282                                            \n",
    "           DEBUG    Metric: 'clip-text-to-clip-vision_average_precision' Value: 0.53372                             \n",
    "           DEBUG    Metric: 'clip-text-to-clip-vision_reciprocal_rank' Value: 0.67706                               \n",
    "           DEBUG    Metric: 'clip-text-to-clip-vision_dcg_at_k' Value: 2.71247                                      \n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3qC3yAcy-Es"
   },
   "source": [
    "## Saving\n",
    "\n",
    "After the run has finished successfully, you can download the tuned model on your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sucF7touyKo0"
   },
   "outputs": [],
   "source": [
    "artifact = run.save_artifact('clip-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_VGjKq3zDx9"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now you saved the `artifact` into your host machine,\n",
    "let's use the fine-tuned model to encode a new `Document`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v95QsuEyzE-B"
   },
   "outputs": [],
   "source": [
    "text_da = DocumentArray([Document(text='some text to encode')])\n",
    "image_da = DocumentArray([Document(\n",
    "    uri='https://upload.wikimedia.org/wikipedia/commons/4/4e/Single_apple.png'\n",
    "    )])\n",
    "\n",
    "clip_text_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
    "clip_image_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')\n",
    "\n",
    "finetuner.encode(model=clip_text_encoder, data=text_da)\n",
    "finetuner.encode(model=clip_image_encoder, data=image_da)\n",
    "\n",
    "print(text_da.embeddings.shape)\n",
    "print(image_da.embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzMbR7VgzXtA"
   },
   "source": [
    "```bash\n",
    "(1, 512)\n",
    "(1, 512)\n",
    "```\n",
    "\n",
    "```{admonition} what is select_model?\n",
    "When fine-tuning CLIP, we are fine-tuning the CLIPVisionEncoder and CLIPTextEncoder in parallel.\n",
    "The artifact contains two models: `clip-vision` and `clip-text`.\n",
    "The parameter `select_model` tells finetuner which model to use for inference, in the above example,\n",
    "we use `clip-text` to encode a Document with text content.\n",
    "```\n",
    "\n",
    "```{admonition} Inference with ONNX\n",
    "In case you set `to_onnx=True` when calling `finetuner.fit` function,\n",
    "please use `model = finetuner.get_model(artifact, is_onnx=True)`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHyMm_M1zxdt"
   },
   "source": [
    "## Advanced: WiSE-FT \n",
    "\n",
    "WiSE-FT, proposed by Mitchell et al. in [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903),\n",
    "has been proven to be an effective way for fine-tuning models with a strong zero-shot capability,\n",
    "such as CLIP.\n",
    "As was introduced in the paper:\n",
    "\n",
    "> Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT).\n",
    "\n",
    "Finetuner allows you to apply WiSE-FT easily,\n",
    "all you need to do is use the `WiSEFTCallback`.\n",
    "Finetuner will trigger the callback when the fine-tuning job is finished and merge the weights between the pre-trained model and the fine-tuned model:\n",
    "\n",
    "```diff\n",
    "from finetuner.callback import WiSEFTCallback\n",
    "\n",
    "run = finetuner.fit(\n",
    "    model='ViT-B-32#openai',\n",
    "    ...,\n",
    "    loss='CLIPLoss',\n",
    "-   callbacks=[],\n",
    "+   callbacks=[WiSEFTCallback(alpha=0.5)],\n",
    ")\n",
    "```\n",
    "\n",
    "The value you set to `alpha` should be greater equal than 0 and less equal than 1:\n",
    "\n",
    "+ if `alpha` is a float between 0 and 1, we merge the weights between the pre-trained model and the fine-tuned model.\n",
    "+ if `alpha` is 0, the fine-tuned model is identical to the pre-trained model.\n",
    "+ if `alpha` is 1, the pre-trained weights will not be utilized.\n",
    "\n",
    "\n",
    "That's it! Check out [clip-as-service](https://clip-as-service.jina.ai/user-guides/finetuner/?highlight=finetuner#fine-tune-models) to learn how to plug-in a fine-tuned CLIP model to our CLIP specific service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before and after\n",
    "We can directly compare the results of our fine-tuned model with a pre-trained clip model by displaying the matches each model has for the same query. While the differences between the results of the two models are quite subtle for some queries, the examples below clearly show that finetuning increases the quality of the search results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "Results for query: \"nightingale tee jacket\" using a zero-shot model (top) and the fine-tuned model (bottom)\n",
    "```\n",
    "![clip-example-pt](images/clip-example-pt.png)\n",
    "\n",
    "![clip-example-ft](images/clip-example-ft.png)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ad9c14fbc5ce15e23594239b0b0bb7cf990b71472055d7d43822c20d61e1cff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
