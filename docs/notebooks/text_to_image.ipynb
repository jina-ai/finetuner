{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-Image Search via CLIP\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1yKnmy2Qotrh3OhgwWRsMWPFwOSAecBxg?usp=sharing\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n",
        "\n",
        "Traditionally, searching images from text (text-image-retrieval) rely heavily on human annotations, this is commonly referred as *Text/Tag based Image Retrieval (TBIR)*.\n",
        "\n",
        "The [OpenAI CLIP](https://github.com/openai/CLIP) model maps the dense vector extracted from text and image into the same semantic space and produced a strong zero-shot model to mearesure the similarity between text and images.\n",
        "\n",
        "This guide will showcase fine-tuning a `CLIP` model for text to image retrieval.\n",
        "\n",
        "*Note, please consider switch to GPU/TPU Runtime for faster inference.*\n",
        "\n",
        "## Install"
      ],
      "metadata": {
        "id": "3UCyCMPcvLGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'finetuner[full]==0.6.4'"
      ],
      "metadata": {
        "id": "vglobi-vvqCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "We'll be fine-tuning CLIP on the [fashion captioning dataset](https://github.com/xuewyang/Fashion_Captioning) which contains information about fashion products.\n",
        "\n",
        "For each product the dataset contains a title and images of multiple variants of the product. We constructed a parent [`Document`](https://docarray.jina.ai/fundamentals/document/#document) for each picture, which contains two [chunks](https://docarray.jina.ai/fundamentals/document/nested/#nested-structure): an image document and a text document holding the description of the product."
      ],
      "metadata": {
        "id": "GXddluSIwCGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "Our journey starts locally. We have to [prepare the data and push it to the Jina AI Cloud](https://finetuner.jina.ai/walkthrough/create-training-data/) and Finetuner will be able to get the dataset by its name. For this example,\n",
        "we already prepared the data, and we'll provide the names of training and evaluation data (`fashion-train-data-clip` and `fashion-eval-data-clip`) directly to Finetuner.\n",
        "\n",
        "```{admonition} \n",
        "We don't require you to push data to the Jina AI Cloud by yourself. Instead of a name, you can provide a `DocumentArray` and Finetuner will do the job for you.\n",
        "```"
      ],
      "metadata": {
        "id": "EVBez7dHwIye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import finetuner\n",
        "from docarray import DocumentArray, Document\n",
        "\n",
        "finetuner.notebook_login(force=True)"
      ],
      "metadata": {
        "id": "vfPZBQVxxEHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DocumentArray.pull('fashion-train-data-clip', show_progress=True)\n",
        "eval_data = DocumentArray.pull('fashion-eval-data-clip', show_progress=True)\n",
        "\n",
        "train_data.summary()"
      ],
      "metadata": {
        "id": "cpIj7viExFti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backbone model\n",
        "Currently, we support several CLIP variations from [open-clip](https://github.com/mlfoundations/open_clip) for text to image retrieval tasks.\n",
        "\n",
        "However, you can see all available models either in [choose backbone](https://finetuner.jina.ai/walkthrough/choose-backbone/) section or by calling `finetuner.describe_models()`."
      ],
      "metadata": {
        "id": "AE87a5Nvwd7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Now that we have the training and evaluation datasets loaded as `DocumentArray`s and selected our model, we can start our fine-tuning run."
      ],
      "metadata": {
        "id": "81fh900Bxgkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import finetuner\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='openai/clip-vit-base-patch32',\n",
        "    train_data='fashion-train-data-clip',\n",
        "    eval_data='fashion-eval-data-clip',\n",
        "    epochs=5,\n",
        "    learning_rate= 1e-7,\n",
        "    loss='CLIPLoss',\n",
        "    device='cuda',\n",
        ")"
      ],
      "metadata": {
        "id": "UDcpfybOv1dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what this piece of code does:\n",
        "\n",
        "* We start with providing `model`, names of training and evaluation data.\n",
        "* We also provide some hyper-parameters such as number of `epochs` and a `learning_rate`.\n",
        "* We use `CLIPLoss` to optimize CLIP model."
      ],
      "metadata": {
        "id": "QPDmFdubxzUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring\n",
        "\n",
        "Now that we've created a run, let's see its status. You can monitor the run by checking the status - `run.status()`, the logs - `run.logs()` or `run.stream_logs()`. "
      ],
      "metadata": {
        "id": "qKv3VcMKyG8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note, the fine-tuning might takes 20~ minutes\n",
        "for entry in run.stream_logs():\n",
        "    print(entry)"
      ],
      "metadata": {
        "id": "JX45y-2fxs4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since some runs might take up to several hours/days, it's important to know how to reconnect to Finetuner and retrieve your run.\n",
        "\n",
        "```python\n",
        "import finetuner\n",
        "\n",
        "finetuner.notebook_login()\n",
        "run = finetuner.get_run(run.name)\n",
        "```\n",
        "\n",
        "You can continue monitoring the run by checking the status - `finetuner.run.Run.status()` or the logs - `finetuner.run.Run.logs()`."
      ],
      "metadata": {
        "id": "xi49YlQsyXbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating\n",
        "Currently, we don't have a user-friendly way to get evaluation metrics from the {class}`~finetuner.callback.EvaluationCallback` we initialized previously.\n",
        "\n",
        "```bash\n",
        "           INFO     Done âœ¨                                                                              __main__.py:219\n",
        "           INFO     Saving fine-tuned models ...                                                         __main__.py:222\n",
        "           INFO     Saving model 'model' in /usr/src/app/tuned-models/model ...                          __main__.py:233\n",
        "           INFO     Pushing saved model to Hubble ...                                                    __main__.py:240\n",
        "[10:38:14] INFO     Pushed model artifact ID: '62a1af491597c219f6a330fe'                                 __main__.py:246\n",
        "           INFO     Finished ðŸš€                                                                          __main__.py:248\n",
        "```\n",
        "\n",
        "```{admonition} Evaluation of CLIP\n",
        "\n",
        "In this example, we did not plug-in an `EvaluationCallback` since the callback can evaluate one model at one time.\n",
        "In most cases, we want to evaluate two models: i.e. use `CLIPTextEncoder` to encode textual Documents as `query_data` while use `CLIPImageEncoder` to encode image Documents as `index_data`.\n",
        "Then use the textual Documents to search image Documents.\n",
        "\n",
        "We have done the evaulation for you in the table below.\n",
        "```\n",
        "\n",
        "\n",
        "|                   | Before Finetuning | After Finetuning |\n",
        "|:------------------|---------:|---------:|\n",
        "| average_precision | 0.47219  | 0.532773 |\n",
        "| dcg_at_k          | 2.25565  | 2.70725  |\n",
        "| f1_score_at_k     | 0.296816 | 0.353499 |\n",
        "| hit_at_k          | 0.94028  | 0.942821 |\n",
        "| ndcg_at_k         | 0.613387 | 0.673644 |\n",
        "| precision_at_k    | 0.240407 | 0.285324 |\n",
        "| r_precision       | 0.364697 | 0.409577 |\n",
        "| recall_at_k       | 0.472681 | 0.564168 |\n",
        "| reciprocal_rank   | 0.575481 | 0.67571  |"
      ],
      "metadata": {
        "id": "Xeq_aVRxyqlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving\n",
        "\n",
        "After the run has finished successfully, you can download the tuned model on your local machine:"
      ],
      "metadata": {
        "id": "h3qC3yAcy-Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artifact = run.save_artifact('clip-model')"
      ],
      "metadata": {
        "id": "sucF7touyKo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Now you saved the `artifact` into your host machine,\n",
        "let's use the fine-tuned model to encode a new `Document`:"
      ],
      "metadata": {
        "id": "8_VGjKq3zDx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_da = DocumentArray([Document(text='some text to encode')])\n",
        "image_da = DocumentArray([Document(uri='my-image.png')])\n",
        "\n",
        "clip_text_encoder = finetuner.get_model(artifact=artifact, select_model='clip-text')\n",
        "clip_image_encoder = finetuner.get_model(artifact=artifact, select_model='clip-vision')\n",
        "\n",
        "finetuner.encode(model=clip_text_encoder, data=text_da)\n",
        "finetuner.encode(model=clip_image_encoder, data=image_da)\n",
        "\n",
        "print(text_da.embeddings.shape)\n",
        "print(image_da.embeddings.shape)"
      ],
      "metadata": {
        "id": "v95QsuEyzE-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "(1, 512)\n",
        "(1, 512)\n",
        "```\n",
        "\n",
        "```{admonition} what is select_model?\n",
        "When fine-tuning CLIP, we are fine-tuning the CLIPVisionEncoder and CLIPTextEncoder in parallel.\n",
        "The artifact contains two models: `clip-vision` and `clip-text`.\n",
        "The parameter `select_model` tells finetuner which model to use for inference, in the above example,\n",
        "we use `clip-text` to encode a Document with text content.\n",
        "```\n",
        "\n",
        "```{admonition} Inference with ONNX\n",
        "In case you set `to_onnx=True` when calling `finetuner.fit` function,\n",
        "please use `model = finetuner.get_model(artifact, is_onnx=True)`\n",
        "```"
      ],
      "metadata": {
        "id": "LzMbR7VgzXtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: WiSE-FT \n",
        "\n",
        "WiSE-FT, proposed by Mitchell et al. in [Robust fine-tuning of zero-shot models](https://arxiv.org/abs/2109.01903),\n",
        "has been proven to be an effective way for fine-tuning models with a strong zero-shot capability,\n",
        "such as CLIP.\n",
        "As was introduced in the paper:\n",
        "\n",
        "> Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT).\n",
        "\n",
        "Finetuner allows you to apply WiSE-FT easily,\n",
        "all you need to do is use the `WiSEFTCallback`.\n",
        "Finetuner will trigger the callback when fine-tuning job finished and merge the weights between the pre-trained model and the fine-tuned model:\n",
        "\n",
        "```diff\n",
        "from finetuner.callbakcs import WiSEFTCallback\n",
        "\n",
        "run = finetuner.fit(\n",
        "    model='ViT-B-32#openai',\n",
        "    ...,\n",
        "    loss='CLIPLoss',\n",
        "-   callbacks=[],\n",
        "+   callbacks=[WiSEFTCallback(alpha=0.5)],\n",
        ")\n",
        "```\n",
        "\n",
        "The value you set to `alpha` should be greater equal than 0 and less equal than 1:\n",
        "\n",
        "+ if `alpha` is a float between 0 and 1, we merge the weights between the pre-trained model and the fine-tuned model.\n",
        "+ if `alpha` is 0, the fine-tuned model is identical to the pre-trained model.\n",
        "+ if `alpha` is 1, the pre-trained weights will not be utilized.\n",
        "\n",
        "\n",
        "That's it! Check out [clip-as-service](https://clip-as-service.jina.ai/user-guides/finetuner/?highlight=finetuner#fine-tune-models) to learn how to plug-in a fine-tuned CLIP model to our CLIP specific service."
      ],
      "metadata": {
        "id": "LHyMm_M1zxdt"
      }
    }
  ]
}