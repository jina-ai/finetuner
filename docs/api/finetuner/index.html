<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="finetuner package" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://finetuner.jina.ai/api/finetuner/" />
<meta property="og:site_name" content="Finetuner Documentation" />
<meta property="og:description" content="Subpackages: finetuner.client package- Submodules- finetuner.client.client module- FinetunerV1Client., finetuner.client.session module., Module contents.. Submodules: finetuner.console module- print_model_table(), print_examples(), print_metrics()., finetuner.data module- CSVOptions- CSVOptions.s..." />
<meta property="og:image" content="https://finetuner.jina.ai/_static/banner.png" />
<meta property="og:image:alt" content="Finetuner Documentation" />
<meta name="description" content="Subpackages: finetuner.client package- Submodules- finetuner.client.client module- FinetunerV1Client., finetuner.client.session module., Module contents.. Submodules: finetuner.console module- print_model_table(), print_examples(), print_metrics()., finetuner.data module- CSVOptions- CSVOptions.s..." />
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@JinaAI_">
<meta name="twitter:creator" content="@JinaAI_">
<meta name="description" content="Finetuner is a library for tune the weights of any deep neural network for better embeddings on search tasks.">
<meta property="og:description" content="Finetuner allows one to tune the weights of any deep neural network for better embeddings on search tasks. It accompanies Jina to deliver the last mile of performance for domain-specific neural search applications.">

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1ESRNDCK35"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-1ESRNDCK35');
</script>

<script async defer src="https://buttons.github.io/buttons.js"></script>
    
<link rel="index" title="Index" href="../../genindex/" /><link rel="search" title="Search" href="../../search/" />
        <link rel="canonical" href="https://finetuner.jina.ai/api/finetuner.html" />

    <link rel="shortcut icon" href="../../_static/favicon.png"/><!-- Generated with Sphinx 7.0.1 and Furo 2023.05.20 -->
        <title>finetuner package - Finetuner documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=e6660623a769aa55fea372102b9bf3151b292993" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta2/css/all.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: #4d4d4d;
  --color-brand-primary: #009191;
  --color-brand-content: #009191;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-brand-primary: #FBCB67;
  --color-brand-content: #FBCB67;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
    <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
    <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
    <header class="mobile-header">
        <div class="header-left">
            <label class="nav-overlay-icon" for="__navigation">
                <div class="visually-hidden">Toggle site navigation sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-menu"></use>
                    </svg>
                </i>
            </label>
        </div>
        <div class="header-center">
            <a href="../../">
                <div class="brand">Finetuner  documentation</div>
            </a>
        </div>
        <div class="header-right">
            <div class="theme-toggle-container theme-toggle-header">
                <button class="theme-toggle">
                    <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                    <svg class="theme-icon-when-auto">
                        <use href="#svg-sun-half"></use>
                    </svg>
                    <svg class="theme-icon-when-dark">
                        <use href="#svg-moon"></use>
                    </svg>
                    <svg class="theme-icon-when-light">
                        <use href="#svg-sun"></use>
                    </svg>
                </button>
            </div>
            <label class="toc-overlay-icon toc-header-icon" for="__toc">
                <div class="visually-hidden">Toggle table of contents sidebar</div>
                <i class="icon">
                    <svg>
                        <use href="#svg-toc"></use>
                    </svg>
                </i>
            </label>
        </div>
    </header>
    <aside class="sidebar-drawer">
        <div class="sidebar-container">
            
            <div class="sidebar-sticky"><a class="sidebar-brand" href="../../">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../../_static/logo-light.svg" alt="Light Logo" />
    <img class="sidebar-logo only-dark" src="../../_static/logo-dark.svg" alt="Dark Logo" />
  </div>
  
  
</a>
<div class="sd-d-flex-row sd-align-major-spaced">
  <a class="github-button" href="https://github.com/jina-ai/finetuner" data-icon="octicon-star" data-show-count="true" aria-label="Star jina-ai/jina on GitHub" style="opacity: 0;">Star</a>
  
</div><form class="sidebar-search-container" method="get" action="../../search/" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
    <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/how-it-works/"><svg aria-hidden="true" class="sd-octicon sd-octicon-question" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm9 3a1 1 0 11-2 0 1 1 0 012 0zM6.92 6.085c.081-.16.19-.299.34-.398.145-.097.371-.187.74-.187.28 0 .553.087.738.225A.613.613 0 019 6.25c0 .177-.04.264-.077.318a.956.956 0 01-.277.245c-.076.051-.158.1-.258.161l-.007.004a7.728 7.728 0 00-.313.195 2.416 2.416 0 00-.692.661.75.75 0 001.248.832.956.956 0 01.276-.245 6.3 6.3 0 01.26-.16l.006-.004c.093-.057.204-.123.313-.195.222-.149.487-.355.692-.662.214-.32.329-.702.329-1.15 0-.76-.36-1.348-.863-1.725A2.76 2.76 0 008 4c-.631 0-1.155.16-1.572.438-.413.276-.68.638-.849.977a.75.75 0 101.342.67z" fill-rule="evenodd"></path></svg> How Does it Work?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get-started/installation/"><svg aria-hidden="true" class="sd-octicon sd-octicon-desktop-download" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4.927 5.427l2.896 2.896a.25.25 0 00.354 0l2.896-2.896A.25.25 0 0010.896 5H8.75V.75a.75.75 0 10-1.5 0V5H5.104a.25.25 0 00-.177.427z"></path><path d="M1.573 2.573a.25.25 0 00-.073.177v7.5a.25.25 0 00.25.25h12.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-3a.75.75 0 110-1.5h3A1.75 1.75 0 0116 2.75v7.5A1.75 1.75 0 0114.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.75.75 0 0111.25 16h-6.5a.75.75 0 01-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 010 10.25v-7.5A1.75 1.75 0 011.75 1h3a.75.75 0 010 1.5h-3a.25.25 0 00-.177.073zM6.982 12a5.72 5.72 0 01-.765 2.5h3.566a5.72 5.72 0 01-.765-2.5H6.982z"></path></svg> Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../walkthrough/"><svg aria-hidden="true" class="sd-octicon sd-octicon-list-ordered" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M2.003 2.5a.5.5 0 00-.723-.447l-1.003.5a.5.5 0 00.446.895l.28-.14V6H.5a.5.5 0 000 1h2.006a.5.5 0 100-1h-.503V2.5zM5 3.25a.75.75 0 01.75-.75h8.5a.75.75 0 010 1.5h-8.5A.75.75 0 015 3.25zm0 5a.75.75 0 01.75-.75h8.5a.75.75 0 010 1.5h-8.5A.75.75 0 015 8.25zm0 5a.75.75 0 01.75-.75h8.5a.75.75 0 010 1.5h-8.5a.75.75 0 01-.75-.75zM.924 10.32l.003-.004a.851.851 0 01.144-.153A.66.66 0 011.5 10c.195 0 .306.068.374.146a.57.57 0 01.128.376c0 .453-.269.682-.8 1.078l-.035.025C.692 11.98 0 12.495 0 13.5a.5.5 0 00.5.5h2.003a.5.5 0 000-1H1.146c.132-.197.351-.372.654-.597l.047-.035c.47-.35 1.156-.858 1.156-1.845 0-.365-.118-.744-.377-1.038-.268-.303-.658-.484-1.126-.484-.48 0-.84.202-1.068.392a1.858 1.858 0 00-.348.384l-.007.011-.002.004-.001.002-.001.001a.5.5 0 00.851.525zM.5 10.055l-.427-.26.427.26z" fill-rule="evenodd"></path></svg> Walkthrough</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of  Walkthrough</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/basic-concepts/">Basic Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/login/">Login</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/create-training-data/">Prepare Training Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/choose-backbone/">Backbone Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/run-job/">Run Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/save-model/">Save Artifact</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../walkthrough/inference/">Inference</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/budget/"><svg aria-hidden="true" class="sd-octicon sd-octicon-database" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M2.5 3.5c0-.133.058-.318.282-.55.227-.237.592-.484 1.1-.708C4.899 1.795 6.354 1.5 8 1.5c1.647 0 3.102.295 4.117.742.51.224.874.47 1.101.707.224.233.282.418.282.551 0 .133-.058.318-.282.55-.227.237-.592.484-1.1.708C11.101 5.205 9.646 5.5 8 5.5c-1.647 0-3.102-.295-4.117-.742-.51-.224-.874-.47-1.101-.707-.224-.233-.282-.418-.282-.551zM1 3.5c0-.626.292-1.165.7-1.59.406-.422.956-.767 1.579-1.041C4.525.32 6.195 0 8 0c1.805 0 3.475.32 4.722.869.622.274 1.172.62 1.578 1.04.408.426.7.965.7 1.591v9c0 .626-.292 1.165-.7 1.59-.406.422-.956.767-1.579 1.041C11.476 15.68 9.806 16 8 16c-1.805 0-3.475-.32-4.721-.869-.623-.274-1.173-.62-1.579-1.04-.408-.426-.7-.965-.7-1.591v-9zM2.5 8V5.724c.241.15.503.286.779.407C4.525 6.68 6.195 7 8 7c1.805 0 3.475-.32 4.722-.869.275-.121.537-.257.778-.407V8c0 .133-.058.318-.282.55-.227.237-.592.484-1.1.708C11.101 9.705 9.646 10 8 10c-1.647 0-3.102-.295-4.117-.742-.51-.224-.874-.47-1.101-.707C2.558 8.318 2.5 8.133 2.5 8zm0 2.225V12.5c0 .133.058.318.282.55.227.237.592.484 1.1.708 1.016.447 2.471.742 4.118.742 1.647 0 3.102-.295 4.117-.742.51-.224.874-.47 1.101-.707.224-.233.282-.418.282-.551v-2.275c-.241.15-.503.285-.778.406-1.247.549-2.917.869-4.722.869-1.805 0-3.475-.32-4.721-.869a6.236 6.236 0 01-.779-.406z" fill-rule="evenodd"></path></svg> How much data?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/negative-mining/"><svg aria-hidden="true" class="sd-octicon sd-octicon-telescope" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M14.184 1.143a1.75 1.75 0 00-2.502-.57L.912 7.916a1.75 1.75 0 00-.53 2.32l.447.775a1.75 1.75 0 002.275.702l11.745-5.656a1.75 1.75 0 00.757-2.451l-1.422-2.464zm-1.657.669a.25.25 0 01.358.081l1.422 2.464a.25.25 0 01-.108.35l-2.016.97-1.505-2.605 1.85-1.26zM9.436 3.92l1.391 2.41-5.42 2.61-.942-1.63 4.97-3.39zM3.222 8.157l-1.466 1a.25.25 0 00-.075.33l.447.775a.25.25 0 00.325.1l1.598-.769-.83-1.436zm6.253 2.306a.75.75 0 00-.944-.252l-1.809.87a.75.75 0 00-.293.253L4.38 14.326a.75.75 0 101.238.848l1.881-2.75v2.826a.75.75 0 001.5 0v-2.826l1.881 2.75a.75.75 0 001.238-.848l-2.644-3.863z" fill-rule="evenodd"></path></svg> Negative Mining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/using-callbacks/"><svg aria-hidden="true" class="sd-octicon sd-octicon-link" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z" fill-rule="evenodd"></path></svg> Using Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/linear-probe/"><svg aria-hidden="true" class="sd-octicon sd-octicon-pin" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M4.456.734a1.75 1.75 0 012.826.504l.613 1.327a3.081 3.081 0 002.084 1.707l2.454.584c1.332.317 1.8 1.972.832 2.94L11.06 10l3.72 3.72a.75.75 0 11-1.061 1.06L10 11.06l-2.204 2.205c-.968.968-2.623.5-2.94-.832l-.584-2.454a3.081 3.081 0 00-1.707-2.084l-1.327-.613a1.75 1.75 0 01-.504-2.826L4.456.734zM5.92 1.866a.25.25 0 00-.404-.072L1.794 5.516a.25.25 0 00.072.404l1.328.613A4.582 4.582 0 015.73 9.63l.584 2.454a.25.25 0 00.42.12l5.47-5.47a.25.25 0 00-.12-.42L9.63 5.73a4.581 4.581 0 01-3.098-2.537L5.92 1.866z" fill-rule="evenodd"></path></svg> Projection Head</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/advanced-losses-optimizers-and-poolers/"><svg aria-hidden="true" class="sd-octicon sd-octicon-mortar-board" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.693 1.066a.75.75 0 01.614 0l7.25 3.25a.75.75 0 010 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 01.133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.75.75 0 01-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 01-.75.75h-3a.75.75 0 01-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 010-1.368l7.25-3.25zM2.583 5L8 7.428 13.416 5 8 2.572 2.583 5zM2.5 11.25c0-.388.125-.611.25-.735a.704.704 0 01.5-.203c.19 0 .37.071.5.203.125.124.25.347.25.735v2.25H2.5v-2.25z" fill-rule="evenodd"></path></svg> Advanced loss functions, optimizers and poolers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-topics/finetuner-executor/"><svg aria-hidden="true" class="sd-octicon sd-octicon-gear" height="1.0em" version="1.1" viewbox="0 0 16 16" width="1.0em"><path d="M7.429 1.525a6.593 6.593 0 011.142 0c.036.003.108.036.137.146l.289 1.105c.147.56.55.967.997 1.189.174.086.341.183.501.29.417.278.97.423 1.53.27l1.102-.303c.11-.03.175.016.195.046.219.31.41.641.573.989.014.031.022.11-.059.19l-.815.806c-.411.406-.562.957-.53 1.456a4.588 4.588 0 010 .582c-.032.499.119 1.05.53 1.456l.815.806c.08.08.073.159.059.19a6.494 6.494 0 01-.573.99c-.02.029-.086.074-.195.045l-1.103-.303c-.559-.153-1.112-.008-1.529.27-.16.107-.327.204-.5.29-.449.222-.851.628-.998 1.189l-.289 1.105c-.029.11-.101.143-.137.146a6.613 6.613 0 01-1.142 0c-.036-.003-.108-.037-.137-.146l-.289-1.105c-.147-.56-.55-.967-.997-1.189a4.502 4.502 0 01-.501-.29c-.417-.278-.97-.423-1.53-.27l-1.102.303c-.11.03-.175-.016-.195-.046a6.492 6.492 0 01-.573-.989c-.014-.031-.022-.11.059-.19l.815-.806c.411-.406.562-.957.53-1.456a4.587 4.587 0 010-.582c.032-.499-.119-1.05-.53-1.456l-.815-.806c-.08-.08-.073-.159-.059-.19a6.44 6.44 0 01.573-.99c.02-.029.086-.075.195-.045l1.103.303c.559.153 1.112.008 1.529-.27.16-.107.327-.204.5-.29.449-.222.851-.628.998-1.189l.289-1.105c.029-.11.101-.143.137-.146zM8 0c-.236 0-.47.01-.701.03-.743.065-1.29.615-1.458 1.261l-.29 1.106c-.017.066-.078.158-.211.224a5.994 5.994 0 00-.668.386c-.123.082-.233.09-.3.071L3.27 2.776c-.644-.177-1.392.02-1.82.63a7.977 7.977 0 00-.704 1.217c-.315.675-.111 1.422.363 1.891l.815.806c.05.048.098.147.088.294a6.084 6.084 0 000 .772c.01.147-.038.246-.088.294l-.815.806c-.474.469-.678 1.216-.363 1.891.2.428.436.835.704 1.218.428.609 1.176.806 1.82.63l1.103-.303c.066-.019.176-.011.299.071.213.143.436.272.668.386.133.066.194.158.212.224l.289 1.106c.169.646.715 1.196 1.458 1.26a8.094 8.094 0 001.402 0c.743-.064 1.29-.614 1.458-1.26l.29-1.106c.017-.066.078-.158.211-.224a5.98 5.98 0 00.668-.386c.123-.082.233-.09.3-.071l1.102.302c.644.177 1.392-.02 1.82-.63.268-.382.505-.789.704-1.217.315-.675.111-1.422-.364-1.891l-.814-.806c-.05-.048-.098-.147-.088-.294a6.1 6.1 0 000-.772c-.01-.147.039-.246.088-.294l.814-.806c.475-.469.679-1.216.364-1.891a7.992 7.992 0 00-.704-1.218c-.428-.609-1.176-.806-1.82-.63l-1.103.303c-.066.019-.176.011-.299-.071a5.991 5.991 0 00-.668-.386c-.133-.066-.194-.158-.212-.224L10.16 1.29C9.99.645 9.444.095 8.701.031A8.094 8.094 0 008 0zm1.5 8a1.5 1.5 0 11-3 0 1.5 1.5 0 013 0zM11 8a3 3 0 11-6 0 3 3 0 016 0z" fill-rule="evenodd"></path></svg> Use FinetunerExecutor inside a Jina Flow</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Finetuning Tasks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/text_to_text/">Text-to-Text Search via BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/image_to_image/">Image-to-Image Search with TripletMarginLoss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/image_to_image_arcface/">Image-to-Image Search with ArcFaceLoss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/text_to_image/">Text-to-Image Search via CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/multilingual_text_to_image/">Multilingual Text-to-Image Search with MultilingualCLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mesh_to_mesh/">3D Mesh-to-3D Mesh Search via PointNet++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/data_synthesis/">Data Synthesis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api-rst/"><span class="fab fa-python"></span> Python API</a></li>
</ul>

    <p class="caption" role="heading"><span class="caption-text">Ecosystem</span></p>
    <ul>
        <li class="toctree-l1">
            <a class="reference external" href="https://docs.jina.ai">
                <img class="sidebar-ecosys-logo only-light-line" src="../../_static/search-light.svg">
                <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/search-dark.svg">
                Jina</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://cloud.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/hub-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/hub-dark.svg">
            Jina Hub</a></li>
        <li class="toctree-l1"><a class="reference internal" href="#">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/finetuner-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/finetuner-dark.svg">
            Finetuner</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://docarray.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/docarray-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/docarray-dark.svg">
            DocArray</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://clip-as-service.jina.ai">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/cas-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/cas-dark.svg">
            CLIP-as-service</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://github.com/jina-ai/jcloud">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/JCloud-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/JCloud-dark.svg">
            JCloud</a></li>
        <li class="toctree-l1"><a class="reference external" href="https://github.com/jina-ai/now">
            <img class="sidebar-ecosys-logo only-light-line" src="../../_static/now-light.svg">
            <img class="sidebar-ecosys-logo only-dark-line" src="../../_static/now-dark.svg">
            NOW</a></li>
    </ul>
</div>
</div>

            </div>
            
        </div>
    </aside>
    <div class="main">
        <div class="content">
            <div class="article-container">
                <a href="#" class="back-to-top muted-link">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
                    </svg>
                    <span>Back to top</span>
                </a>
                <div class="content-icon-container"><div class="theme-toggle-container theme-toggle-content">
                        <button class="theme-toggle">
                            <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
                            <svg class="theme-icon-when-auto">
                                <use href="#svg-sun-half"></use>
                            </svg>
                            <svg class="theme-icon-when-dark">
                                <use href="#svg-moon"></use>
                            </svg>
                            <svg class="theme-icon-when-light">
                                <use href="#svg-sun"></use>
                            </svg>
                        </button>
                    </div>
                    <label class="toc-overlay-icon toc-content-icon"
                           for="__toc">
                        <div class="visually-hidden">Toggle table of contents sidebar</div>
                        <i class="icon">
                            <svg>
                                <use href="#svg-toc"></use>
                            </svg>
                        </i>
                    </label>
                </div>
                <article role="main">
                    <section id="finetuner-package">
<h1>finetuner package<a class="headerlink" href="#finetuner-package" title="Permalink to this heading">#</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.client/">finetuner.client package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.client/#submodules">Submodules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.client.client/">finetuner.client.client module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../finetuner.client.client/#finetuner.client.client.FinetunerV1Client"><code class="docutils literal notranslate"><span class="pre">FinetunerV1Client</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.client.session/">finetuner.client.session module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.client/#module-finetuner.client">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.console/">finetuner.console module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.console/#finetuner.console.print_model_table"><code class="docutils literal notranslate"><span class="pre">print_model_table()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.console/#finetuner.console.print_examples"><code class="docutils literal notranslate"><span class="pre">print_examples()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.console/#finetuner.console.print_metrics"><code class="docutils literal notranslate"><span class="pre">print_metrics()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.data/">finetuner.data module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions"><code class="docutils literal notranslate"><span class="pre">CSVOptions</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.size"><code class="docutils literal notranslate"><span class="pre">CSVOptions.size</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.sampling_rate"><code class="docutils literal notranslate"><span class="pre">CSVOptions.sampling_rate</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.dialect"><code class="docutils literal notranslate"><span class="pre">CSVOptions.dialect</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.encoding"><code class="docutils literal notranslate"><span class="pre">CSVOptions.encoding</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.is_labeled"><code class="docutils literal notranslate"><span class="pre">CSVOptions.is_labeled</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.convert_to_blob"><code class="docutils literal notranslate"><span class="pre">CSVOptions.convert_to_blob</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.create_point_clouds"><code class="docutils literal notranslate"><span class="pre">CSVOptions.create_point_clouds</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions.point_cloud_size"><code class="docutils literal notranslate"><span class="pre">CSVOptions.point_cloud_size</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.LabeledCSVParser"><code class="docutils literal notranslate"><span class="pre">LabeledCSVParser</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.LabeledCSVParser.parse"><code class="docutils literal notranslate"><span class="pre">LabeledCSVParser.parse()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.QueryDocumentRelationsParser"><code class="docutils literal notranslate"><span class="pre">QueryDocumentRelationsParser</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.QueryDocumentRelationsParser.parse"><code class="docutils literal notranslate"><span class="pre">QueryDocumentRelationsParser.parse()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.PairwiseScoreParser"><code class="docutils literal notranslate"><span class="pre">PairwiseScoreParser</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.PairwiseScoreParser.parse"><code class="docutils literal notranslate"><span class="pre">PairwiseScoreParser.parse()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.DataSynthesisParser"><code class="docutils literal notranslate"><span class="pre">DataSynthesisParser</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.DataSynthesisParser.parse"><code class="docutils literal notranslate"><span class="pre">DataSynthesisParser.parse()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVContext"><code class="docutils literal notranslate"><span class="pre">CSVContext</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.data/#finetuner.data.CSVContext.build_dataset"><code class="docutils literal notranslate"><span class="pre">CSVContext.build_dataset()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.get_csv_file_context"><code class="docutils literal notranslate"><span class="pre">get_csv_file_context()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.get_csv_file_dialect_columns"><code class="docutils literal notranslate"><span class="pre">get_csv_file_dialect_columns()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.build_encoding_dataset"><code class="docutils literal notranslate"><span class="pre">build_encoding_dataset()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.check_columns"><code class="docutils literal notranslate"><span class="pre">check_columns()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.data/#finetuner.data.create_document"><code class="docutils literal notranslate"><span class="pre">create_document()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.excepts/">finetuner.excepts module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.excepts/#finetuner.excepts.FinetunerServerError"><code class="docutils literal notranslate"><span class="pre">FinetunerServerError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.excepts/#finetuner.excepts.RunInProgressError"><code class="docutils literal notranslate"><span class="pre">RunInProgressError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.excepts/#finetuner.excepts.RunPreparingError"><code class="docutils literal notranslate"><span class="pre">RunPreparingError</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.excepts/#finetuner.excepts.RunFailedError"><code class="docutils literal notranslate"><span class="pre">RunFailedError</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.experiment/">finetuner.experiment module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment"><code class="docutils literal notranslate"><span class="pre">Experiment</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.name"><code class="docutils literal notranslate"><span class="pre">Experiment.name</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.status"><code class="docutils literal notranslate"><span class="pre">Experiment.status</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.get_run"><code class="docutils literal notranslate"><span class="pre">Experiment.get_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.list_runs"><code class="docutils literal notranslate"><span class="pre">Experiment.list_runs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.delete_run"><code class="docutils literal notranslate"><span class="pre">Experiment.delete_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.delete_runs"><code class="docutils literal notranslate"><span class="pre">Experiment.delete_runs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.create_training_run"><code class="docutils literal notranslate"><span class="pre">Experiment.create_training_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment.create_synthesis_run"><code class="docutils literal notranslate"><span class="pre">Experiment.create_synthesis_run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.finetuner/">finetuner.finetuner module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner"><code class="docutils literal notranslate"><span class="pre">Finetuner</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.login"><code class="docutils literal notranslate"><span class="pre">Finetuner.login()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.create_experiment"><code class="docutils literal notranslate"><span class="pre">Finetuner.create_experiment()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.get_experiment"><code class="docutils literal notranslate"><span class="pre">Finetuner.get_experiment()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.list_experiments"><code class="docutils literal notranslate"><span class="pre">Finetuner.list_experiments()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.delete_experiment"><code class="docutils literal notranslate"><span class="pre">Finetuner.delete_experiment()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.delete_experiments"><code class="docutils literal notranslate"><span class="pre">Finetuner.delete_experiments()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.create_training_run"><code class="docutils literal notranslate"><span class="pre">Finetuner.create_training_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.create_synthesis_run"><code class="docutils literal notranslate"><span class="pre">Finetuner.create_synthesis_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.get_run"><code class="docutils literal notranslate"><span class="pre">Finetuner.get_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.list_runs"><code class="docutils literal notranslate"><span class="pre">Finetuner.list_runs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.delete_run"><code class="docutils literal notranslate"><span class="pre">Finetuner.delete_run()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.delete_runs"><code class="docutils literal notranslate"><span class="pre">Finetuner.delete_runs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.finetuner/#finetuner.finetuner.Finetuner.get_token"><code class="docutils literal notranslate"><span class="pre">Finetuner.get_token()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.model/">finetuner.model module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.model/#finetuner.model.get_header"><code class="docutils literal notranslate"><span class="pre">get_header()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.model/#finetuner.model.get_row"><code class="docutils literal notranslate"><span class="pre">get_row()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.model/#finetuner.model.list_model_classes"><code class="docutils literal notranslate"><span class="pre">list_model_classes()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.model/#finetuner.model.SynthesisModels"><code class="docutils literal notranslate"><span class="pre">SynthesisModels</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.model/#finetuner.model.SynthesisModels.relation_miner"><code class="docutils literal notranslate"><span class="pre">SynthesisModels.relation_miner</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.model/#finetuner.model.SynthesisModels.cross_encoder"><code class="docutils literal notranslate"><span class="pre">SynthesisModels.cross_encoder</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../finetuner.run/">finetuner.run module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run"><code class="docutils literal notranslate"><span class="pre">Run</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.name"><code class="docutils literal notranslate"><span class="pre">Run.name</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.config"><code class="docutils literal notranslate"><span class="pre">Run.config</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.train_data"><code class="docutils literal notranslate"><span class="pre">Run.train_data</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.status"><code class="docutils literal notranslate"><span class="pre">Run.status()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.logs"><code class="docutils literal notranslate"><span class="pre">Run.logs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.stream_logs"><code class="docutils literal notranslate"><span class="pre">Run.stream_logs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.metrics"><code class="docutils literal notranslate"><span class="pre">Run.metrics()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.display_metrics"><code class="docutils literal notranslate"><span class="pre">Run.display_metrics()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.example_results"><code class="docutils literal notranslate"><span class="pre">Run.example_results()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.display_examples"><code class="docutils literal notranslate"><span class="pre">Run.display_examples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.save_artifact"><code class="docutils literal notranslate"><span class="pre">Run.save_artifact()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../finetuner.run/#finetuner.run.Run.artifact_id"><code class="docutils literal notranslate"><span class="pre">Run.artifact_id</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
<section id="module-finetuner">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-finetuner" title="Permalink to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="finetuner.login">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">login</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">force</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">interactive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#login"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.login" title="Permalink to this definition">#</a></dt>
<dd><p>Login to Jina AI Cloud to use cloud-based fine-tuning.
Thereby, an authentication token is
generated which can be read with the <a class="reference internal" href="#finetuner.get_token" title="finetuner.get_token"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_token()</span></code></a> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>force</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If set to true, an existing token will be overwritten. Otherwise,
you will not login again, if a valid token already exists.</p></li>
<li><p><strong>interactive</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Interactive mode should be set in Jupyter environments.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.list_callbacks">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">list_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#list_callbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.list_callbacks" title="Permalink to this definition">#</a></dt>
<dd><p>List available callbacks.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, ~CallbackStubType]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.list_models">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">list_models</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#list_models"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.list_models" title="Permalink to this definition">#</a></dt>
<dd><p>List available models.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.list_model_options">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">list_model_options</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#list_model_options"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.list_model_options" title="Permalink to this definition">#</a></dt>
<dd><p>List available options per model.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.describe_models">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">describe_models</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">task</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#describe_models"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.describe_models" title="Permalink to this definition">#</a></dt>
<dd><p>Print model information, such as name, task, output dimension, architecture
and description as a table.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>task</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The task for the backbone model, one of <cite>text-to-text</cite>,
<cite>text-to-image</cite>, <cite>image-to-image</cite>. If not provided, will print all backbone
models.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.fit">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_artifact</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">description</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'TripletMarginLoss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_onnx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">csv_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">public</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_items_per_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#finetuner.fit" title="Permalink to this definition">#</a></dt>
<dd><p>Create a Finetuner training <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>, calling this function will submit a
fine-tuning job to the Jina AI Cloud.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of model to be fine-tuned. Run <cite>finetuner.list_models()</cite> or
<cite>finetuner.describe_models()</cite> to see the available model names.</p></li>
<li><p><strong>train_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <cite>DocumentArray</cite> for training data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>eval_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Either a <cite>DocumentArray</cite> for evaluation data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>val_split</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Determines which portion of the <cite>train_data</cite> is held out
for calculating a validation loss. If it is set to 0, or an <cite>eval_data</cite>
parameter is provided, no data is held out from the training data. Instead, the
<cite>eval_data</cite> is used to calculate the validation loss if it is provided.</p></li>
<li><p><strong>model_artifact</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – To continue training the training of a model which was
fine-tuned by a previous run, you can provide the artifact id of this model,
which you can get via <code class="xref py py-meth docutils literal notranslate"><span class="pre">Run.artifact_id()</span></code>.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the run.</p></li>
<li><p><strong>description</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Run description.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the experiment.</p></li>
<li><p><strong>model_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional arguments to pass to the model construction. These
are model specific options and are different depending on the model you choose.
Run <cite>finetuner.list_model_options()</cite> to see available options for every model.</p></li>
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the loss function used for fine-tuning. Default is
<cite>TripletMarginLoss</cite>. Options: <cite>CosFaceLoss</cite>, <cite>NTXLoss</cite>, <cite>AngularLoss</cite>,
<cite>ArcFaceLoss</cite>, <cite>BaseMetricLossFunction</cite>, <cite>MultipleLosses</cite>,
<cite>CentroidTripletLoss</cite>, <cite>CircleLoss</cite>, <cite>ContrastiveLoss</cite>, <cite>CrossBatchMemory</cite>,
<cite>FastAPLoss</cite>, <cite>GenericPairLoss</cite>, <cite>IntraPairVarianceLoss</cite>,
<cite>LargeMarginSoftmaxLoss</cite>, <cite>GeneralizedLiftedStructureLoss</cite>,
<cite>LiftedStructureLoss</cite>, <cite>MarginLoss</cite>, <cite>EmbeddingRegularizerMixin</cite>,
<cite>WeightRegularizerMixin</cite>, <cite>MultiSimilarityLoss</cite>, <cite>NPairsLoss</cite>, <cite>NCALoss</cite>,
<cite>NormalizedSoftmaxLoss</cite>, <cite>ProxyAnchorLoss</cite>, <cite>ProxyNCALoss</cite>,
<cite>SignalToNoiseRatioContrastiveLoss</cite>, <cite>SoftTripleLoss</cite>, <cite>SphereFaceLoss</cite>,
<cite>SupConLoss</cite>, <cite>TripletMarginLoss</cite>, <cite>TupletMarginLoss</cite>, <cite>VICRegLoss</cite>,
<cite>CLIPLoss</cite>.</p></li>
<li><p><strong>miner</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the miner to create tuple indices for the loss function.
Options: <cite>AngularMiner</cite>, <cite>BaseMiner</cite>, <cite>BaseSubsetBatchMiner</cite>, <cite>BaseTupleMiner</cite>,
<cite>BatchEasyHardMiner</cite>, <cite>BatchHardMiner</cite>, <cite>DistanceWeightedMiner</cite>, <cite>HDCMiner</cite>,
<cite>EmbeddingsAlreadyPackagedAsTriplets</cite>, <cite>MaximumLossMiner</cite>, <cite>PairMarginMiner</cite>,
<cite>MultiSimilarityMiner</cite>, <cite>TripletMarginMiner</cite>, <cite>UniformHistogramMiner</cite>.</p></li>
<li><p><strong>miner_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional parameters to pass to the miner construction. The
set of applicable parameters is specific to the miner you choose. Details on
the parameters can be found in the <a class="reference external" href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/">PyTorch Metric Learning documentation</a></p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the optimizer used for fine-tuning. Options: <cite>Adadelta</cite>,
<cite>Adagrad</cite>, <cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>,
<cite>RAdam</cite>, <cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>.</p></li>
<li><p><strong>optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional parameters to pass to the optimizer
construction. The set of applicable parameters is specific to the optimizer you
choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a></p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – learning rate for the optimizer.</p></li>
<li><p><strong>epochs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of epochs for fine-tuning.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Number of items to include in a batch. If not set, the
batch size will be configured automatically.</p></li>
<li><p><strong>callbacks</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[~CallbackStubType]]) – List of callback stub objects.
subpackage for available options, or run <cite>finetuner.list_callbacks()</cite>.</p></li>
<li><p><strong>scheduler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of a scheduler to use for learning rate scheduling.
Supported types are: <cite>linear</cite>, <cite>cosine</cite>, <cite>cosine_with_restarts</cite>, <cite>polynomial</cite>,
<cite>constant</cite>, <cite>constant_with_warmup</cite>.</p></li>
<li><p><strong>scheduler_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Dictionary of additional parameters to pass to the
scheduler: <cite>num_warmup_steps</cite>, <cite>num_training_steps</cite>, and <cite>scheduler_step</cite>
(either <cite>batch</cite> or <cite>epoch</cite>).</p></li>
<li><p><strong>freeze</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If set to <cite>True</cite>, will freeze all layers except the last one.</p></li>
<li><p><strong>output_dim</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The expected output dimension as <cite>int</cite>.
If set, will attach a projection head.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>num_workers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of CPU workers. If <cite>cpu: False</cite> this is the number of
workers used by the dataloader.</p></li>
<li><p><strong>to_onnx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Set this parameter as <cite>True</cite> to convert the model to an onnx model.
Please note that not all models support this. If this parameter is set, please
pass <cite>is_onnx</cite> when making inference, e.g., when calling the <cite>get_model</cite>
function.</p></li>
<li><p><strong>csv_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions" title="finetuner.data.CSVOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code></a>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code> object containing options used for
reading in training and evaluation data from a CSV file, if they are
provided as such.</p></li>
<li><p><strong>public</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – A boolean value indicates if the artifact is public. It should be
set to <cite>True</cite> if you would like to share your fine-tuned model with others.</p></li>
<li><p><strong>num_items_per_class</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many items per class (unique labels) to include
in a batch. For example, if <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is 20, and
<code class="docutils literal notranslate"><span class="pre">num_items_per_class</span></code> is 4, the batch will consist of 4 items for each of
the 5 classes. Batch size must be divisible by <cite>num_items_per_class</cite>.</p></li>
<li><p><strong>sampler</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Determines which sampling method will be used if the
data is labeled. Default is <cite>auto</cite>, meaning that the sampler,
will be the default for the loss function used.
Setting to <cite>class</cite> will result in the <cite>ClassSampler</cite> being used, and setting to
<cite>random</cite> will result in the <cite>RandomSampler</cite> being used.
If set to <cite>random</cite> then <cite>num_items_per_class</cite> is not used.</p></li>
<li><p><strong>loss_optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the optimizer used for fine-tuning to loss function,
if it is a function that requires an optimizer. Options: <cite>Adadelta</cite>, <cite>Adagrad</cite>,
<cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>, <cite>RAdam</cite>,
<cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>. If left as None then optimizer specified by
the <cite>optimizer</cite> argument will be used instead.</p></li>
<li><p><strong>loss_optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the optimizer of
the loss function. The set of applicable parameters is specific to the optimizer
you choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a>.</p>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless necessary, please stick with <cite>device=”cuda”</cite>, <cite>cpu</cite> training could be
extremely slow and inefficient.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.synthesize">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">synthesize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corpus_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_relations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">description</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">csv_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">public</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#finetuner.synthesize" title="Permalink to this definition">#</a></dt>
<dd><p>Create a Finetuner synthesis <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>, calling this function will submit a
data synthesis job to the Jina AI Cloud.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code> for example queries, name of a
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud, the dataset itself as
a list of strings or a path to a CSV file.</p></li>
<li><p><strong>corpus_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code> for corpus data, a name of a
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud, the dataset itself as a
list of strings or a path to a CSV file.</p></li>
<li><p><strong>models</strong> (<a class="reference internal" href="../finetuner.model/#finetuner.model.SynthesisModels" title="finetuner.model.SynthesisModels"><code class="xref py py-class docutils literal notranslate"><span class="pre">SynthesisModels</span></code></a>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">SynthesisModels</span></code> object containing the names of
the models used for relation mining and cross encoding.
You can pass <cite>finetuner.data.DATA_SYNTHESIS_EN</cite> for the recommended models for
synthesis based on english data.</p></li>
<li><p><strong>num_relations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of relations to mine per query.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the run.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the experiment.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>num_workers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of CPU workers. If <cite>cpu: False</cite> this is the number of
workers used by the dataloader.</p></li>
<li><p><strong>csv_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions" title="finetuner.data.CSVOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code></a>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code> object containing options used for
reading in training and evaluation data from a CSV file, if they are
provided as such.</p></li>
<li><p><strong>public</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – A boolean value indicates if the artifact is public. It should be
set to <cite>True</cite> if you would like to share your synthesized data with others.</p></li>
</ul>
</dd>
<dt class="field-even">Param<span class="colon">:</span></dt>
<dd class="field-even"><p>description: Run Description.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless necessary, please stick with <cite>device=”cuda”</cite>, <cite>cpu</cite> training could be
extremely slow and inefficient.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.create_training_run">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">create_training_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_artifact</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">description</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'TripletMarginLoss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_onnx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">csv_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">public</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_items_per_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#finetuner.create_training_run" title="Permalink to this definition">#</a></dt>
<dd><p>Create a Finetuner training <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>, calling this function will submit a
fine-tuning job to the Jina AI Cloud.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of model to be fine-tuned. Run <cite>finetuner.list_models()</cite> or
<cite>finetuner.describe_models()</cite> to see the available model names.</p></li>
<li><p><strong>train_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <cite>DocumentArray</cite> for training data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>eval_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Either a <cite>DocumentArray</cite> for evaluation data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>val_split</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Determines which portion of the <cite>train_data</cite> is held out
for calculating a validation loss. If it is set to 0, or an <cite>eval_data</cite>
parameter is provided, no data is held out from the training data. Instead, the
<cite>eval_data</cite> is used to calculate the validation loss if it is provided.</p></li>
<li><p><strong>model_artifact</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – To continue training the training of a model which was
fine-tuned by a previous run, you can provide the artifact id of this model,
which you can get via <code class="xref py py-meth docutils literal notranslate"><span class="pre">Run.artifact_id()</span></code>.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the run.</p></li>
<li><p><strong>description</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Run description.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the experiment.</p></li>
<li><p><strong>model_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional arguments to pass to the model construction. These
are model specific options and are different depending on the model you choose.
Run <cite>finetuner.list_model_options()</cite> to see available options for every model.</p></li>
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the loss function used for fine-tuning. Default is
<cite>TripletMarginLoss</cite>. Options: <cite>CosFaceLoss</cite>, <cite>NTXLoss</cite>, <cite>AngularLoss</cite>,
<cite>ArcFaceLoss</cite>, <cite>BaseMetricLossFunction</cite>, <cite>MultipleLosses</cite>,
<cite>CentroidTripletLoss</cite>, <cite>CircleLoss</cite>, <cite>ContrastiveLoss</cite>, <cite>CrossBatchMemory</cite>,
<cite>FastAPLoss</cite>, <cite>GenericPairLoss</cite>, <cite>IntraPairVarianceLoss</cite>,
<cite>LargeMarginSoftmaxLoss</cite>, <cite>GeneralizedLiftedStructureLoss</cite>,
<cite>LiftedStructureLoss</cite>, <cite>MarginLoss</cite>, <cite>EmbeddingRegularizerMixin</cite>,
<cite>WeightRegularizerMixin</cite>, <cite>MultiSimilarityLoss</cite>, <cite>NPairsLoss</cite>, <cite>NCALoss</cite>,
<cite>NormalizedSoftmaxLoss</cite>, <cite>ProxyAnchorLoss</cite>, <cite>ProxyNCALoss</cite>,
<cite>SignalToNoiseRatioContrastiveLoss</cite>, <cite>SoftTripleLoss</cite>, <cite>SphereFaceLoss</cite>,
<cite>SupConLoss</cite>, <cite>TripletMarginLoss</cite>, <cite>TupletMarginLoss</cite>, <cite>VICRegLoss</cite>,
<cite>CLIPLoss</cite>.</p></li>
<li><p><strong>miner</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the miner to create tuple indices for the loss function.
Options: <cite>AngularMiner</cite>, <cite>BaseMiner</cite>, <cite>BaseSubsetBatchMiner</cite>, <cite>BaseTupleMiner</cite>,
<cite>BatchEasyHardMiner</cite>, <cite>BatchHardMiner</cite>, <cite>DistanceWeightedMiner</cite>, <cite>HDCMiner</cite>,
<cite>EmbeddingsAlreadyPackagedAsTriplets</cite>, <cite>MaximumLossMiner</cite>, <cite>PairMarginMiner</cite>,
<cite>MultiSimilarityMiner</cite>, <cite>TripletMarginMiner</cite>, <cite>UniformHistogramMiner</cite>.</p></li>
<li><p><strong>miner_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the miner construction. The
set of applicable parameters is specific to the miner you choose. Details on
the parameters can be found in the <a class="reference external" href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/">PyTorch Metric Learning documentation</a></p>
</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the optimizer used for fine-tuning. Options: <cite>Adadelta</cite>,
<cite>Adagrad</cite>, <cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>,
<cite>RAdam</cite>, <cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>.</p></li>
<li><p><strong>optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the optimizer
construction. The set of applicable parameters is specific to the optimizer you
choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a></p>
</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – learning rate for the optimizer.</p></li>
<li><p><strong>epochs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of epochs for fine-tuning.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Number of items to include in a batch. If not set, the
batch size will be configured automatically.</p></li>
<li><p><strong>callbacks</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[~CallbackStubType]]) – List of callback stub objects.
subpackage for available options, or run <cite>finetuner.list_callbacks()</cite>.</p></li>
<li><p><strong>scheduler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of a scheduler to use for learning rate scheduling.
Supported types are: <cite>linear</cite>, <cite>cosine</cite>, <cite>cosine_with_restarts</cite>, <cite>polynomial</cite>,
<cite>constant</cite>, <cite>constant_with_warmup</cite>.</p></li>
<li><p><strong>scheduler_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Dictionary of additional parameters to pass to the
scheduler: <cite>num_warmup_steps</cite>, <cite>num_training_steps</cite>, and <cite>scheduler_step</cite>
(either <cite>batch</cite> or <cite>epoch</cite>).</p></li>
<li><p><strong>freeze</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If set to <cite>True</cite>, will freeze all layers except the last one.</p></li>
<li><p><strong>output_dim</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The expected output dimension as <cite>int</cite>.
If set, will attach a projection head.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>num_workers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of CPU workers. If <cite>cpu: False</cite> this is the number of
workers used by the dataloader.</p></li>
<li><p><strong>to_onnx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Set this parameter as <cite>True</cite> to convert the model to an onnx model.
Please note that not all models support this. If this parameter is set, please
pass <cite>is_onnx</cite> when making inference, e.g., when calling the <cite>get_model</cite>
function.</p></li>
<li><p><strong>csv_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions" title="finetuner.data.CSVOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code></a>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code> object containing options used for
reading in training and evaluation data from a CSV file, if they are
provided as such.</p></li>
<li><p><strong>public</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – A boolean value indicates if the artifact is public. It should be
set to <cite>True</cite> if you would like to share your fine-tuned model with others.</p></li>
<li><p><strong>num_items_per_class</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many items per class (unique labels) to include
in a batch. For example, if <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is 20, and
<code class="docutils literal notranslate"><span class="pre">num_items_per_class</span></code> is 4, the batch will consist of 4 items for each of
the 5 classes. Batch size must be divisible by <cite>num_items_per_class</cite>.</p></li>
<li><p><strong>sampler</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Determines which sampling method will be used if the
data is labeled. Default is <cite>auto</cite>, meaning that the sampler,
will be the default for the loss function used.
Setting to <cite>class</cite> will result in the <cite>ClassSampler</cite> being used, and setting to
<cite>random</cite> will result in the <cite>RandomSampler</cite> being used.
If set to <cite>random</cite> then <cite>num_items_per_class</cite> is not used.</p></li>
<li><p><strong>loss_optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the optimizer used for fine-tuning to loss function,
if it is a function that requires an optimizer. Options: <cite>Adadelta</cite>, <cite>Adagrad</cite>,
<cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>, <cite>RAdam</cite>,
<cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>. If left as None then optimizer specified by
the <cite>optimizer</cite> argument will be used instead.</p></li>
<li><p><strong>loss_optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the optimizer of
the loss function. The set of applicable parameters is specific to the optimizer
you choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a>.</p>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless necessary, please stick with <cite>device=”cuda”</cite>, <cite>cpu</cite> training could be
extremely slow and inefficient.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.create_run">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">create_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_artifact</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">description</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'TripletMarginLoss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">miner_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">to_onnx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">csv_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">public</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_items_per_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_optimizer_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#finetuner.create_run" title="Permalink to this definition">#</a></dt>
<dd><p>Create a Finetuner training <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>, calling this function will submit a
fine-tuning job to the Jina AI Cloud.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of model to be fine-tuned. Run <cite>finetuner.list_models()</cite> or
<cite>finetuner.describe_models()</cite> to see the available model names.</p></li>
<li><p><strong>train_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <cite>DocumentArray</cite> for training data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>eval_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">TextIO</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Either a <cite>DocumentArray</cite> for evaluation data, a name of the
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud or a path to a CSV file.</p></li>
<li><p><strong>val_split</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – Determines which portion of the <cite>train_data</cite> is held out
for calculating a validation loss. If it is set to 0, or an <cite>eval_data</cite>
parameter is provided, no data is held out from the training data. Instead, the
<cite>eval_data</cite> is used to calculate the validation loss if it is provided.</p></li>
<li><p><strong>model_artifact</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – To continue training the training of a model which was
fine-tuned by a previous run, you can provide the artifact id of this model,
which you can get via <code class="xref py py-meth docutils literal notranslate"><span class="pre">Run.artifact_id()</span></code>.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the run.</p></li>
<li><p><strong>description</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Run description.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the experiment.</p></li>
<li><p><strong>model_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional arguments to pass to the model construction. These
are model specific options and are different depending on the model you choose.
Run <cite>finetuner.list_model_options()</cite> to see available options for every model.</p></li>
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the loss function used for fine-tuning. Default is
<cite>TripletMarginLoss</cite>. Options: <cite>CosFaceLoss</cite>, <cite>NTXLoss</cite>, <cite>AngularLoss</cite>,
<cite>ArcFaceLoss</cite>, <cite>BaseMetricLossFunction</cite>, <cite>MultipleLosses</cite>,
<cite>CentroidTripletLoss</cite>, <cite>CircleLoss</cite>, <cite>ContrastiveLoss</cite>, <cite>CrossBatchMemory</cite>,
<cite>FastAPLoss</cite>, <cite>GenericPairLoss</cite>, <cite>IntraPairVarianceLoss</cite>,
<cite>LargeMarginSoftmaxLoss</cite>, <cite>GeneralizedLiftedStructureLoss</cite>,
<cite>LiftedStructureLoss</cite>, <cite>MarginLoss</cite>, <cite>EmbeddingRegularizerMixin</cite>,
<cite>WeightRegularizerMixin</cite>, <cite>MultiSimilarityLoss</cite>, <cite>NPairsLoss</cite>, <cite>NCALoss</cite>,
<cite>NormalizedSoftmaxLoss</cite>, <cite>ProxyAnchorLoss</cite>, <cite>ProxyNCALoss</cite>,
<cite>SignalToNoiseRatioContrastiveLoss</cite>, <cite>SoftTripleLoss</cite>, <cite>SphereFaceLoss</cite>,
<cite>SupConLoss</cite>, <cite>TripletMarginLoss</cite>, <cite>TupletMarginLoss</cite>, <cite>VICRegLoss</cite>,
<cite>CLIPLoss</cite>.</p></li>
<li><p><strong>miner</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the miner to create tuple indices for the loss function.
Options: <cite>AngularMiner</cite>, <cite>BaseMiner</cite>, <cite>BaseSubsetBatchMiner</cite>, <cite>BaseTupleMiner</cite>,
<cite>BatchEasyHardMiner</cite>, <cite>BatchHardMiner</cite>, <cite>DistanceWeightedMiner</cite>, <cite>HDCMiner</cite>,
<cite>EmbeddingsAlreadyPackagedAsTriplets</cite>, <cite>MaximumLossMiner</cite>, <cite>PairMarginMiner</cite>,
<cite>MultiSimilarityMiner</cite>, <cite>TripletMarginMiner</cite>, <cite>UniformHistogramMiner</cite>.</p></li>
<li><p><strong>miner_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the miner construction. The
set of applicable parameters is specific to the miner you choose. Details on
the parameters can be found in the <a class="reference external" href="https://kevinmusgrave.github.io/pytorch-metric-learning/miners/">PyTorch Metric Learning documentation</a></p>
</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the optimizer used for fine-tuning. Options: <cite>Adadelta</cite>,
<cite>Adagrad</cite>, <cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>,
<cite>RAdam</cite>, <cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>.</p></li>
<li><p><strong>optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the optimizer
construction. The set of applicable parameters is specific to the optimizer you
choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a></p>
</p></li>
<li><p><strong>learning_rate</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – learning rate for the optimizer.</p></li>
<li><p><strong>epochs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of epochs for fine-tuning.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Number of items to include in a batch. If not set, the
batch size will be configured automatically.</p></li>
<li><p><strong>callbacks</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[~CallbackStubType]]) – List of callback stub objects.
subpackage for available options, or run <cite>finetuner.list_callbacks()</cite>.</p></li>
<li><p><strong>scheduler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of a scheduler to use for learning rate scheduling.
Supported types are: <cite>linear</cite>, <cite>cosine</cite>, <cite>cosine_with_restarts</cite>, <cite>polynomial</cite>,
<cite>constant</cite>, <cite>constant_with_warmup</cite>.</p></li>
<li><p><strong>scheduler_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Dictionary of additional parameters to pass to the
scheduler: <cite>num_warmup_steps</cite>, <cite>num_training_steps</cite>, and <cite>scheduler_step</cite>
(either <cite>batch</cite> or <cite>epoch</cite>).</p></li>
<li><p><strong>freeze</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If set to <cite>True</cite>, will freeze all layers except the last one.</p></li>
<li><p><strong>output_dim</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The expected output dimension as <cite>int</cite>.
If set, will attach a projection head.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>num_workers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of CPU workers. If <cite>cpu: False</cite> this is the number of
workers used by the dataloader.</p></li>
<li><p><strong>to_onnx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Set this parameter as <cite>True</cite> to convert the model to an onnx model.
Please note that not all models support this. If this parameter is set, please
pass <cite>is_onnx</cite> when making inference, e.g., when calling the <cite>get_model</cite>
function.</p></li>
<li><p><strong>csv_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions" title="finetuner.data.CSVOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code></a>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code> object containing options used for
reading in training and evaluation data from a CSV file, if they are
provided as such.</p></li>
<li><p><strong>public</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – A boolean value indicates if the artifact is public. It should be
set to <cite>True</cite> if you would like to share your fine-tuned model with others.</p></li>
<li><p><strong>num_items_per_class</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many items per class (unique labels) to include
in a batch. For example, if <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is 20, and
<code class="docutils literal notranslate"><span class="pre">num_items_per_class</span></code> is 4, the batch will consist of 4 items for each of
the 5 classes. Batch size must be divisible by <cite>num_items_per_class</cite>.</p></li>
<li><p><strong>sampler</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Determines which sampling method will be used if the
data is labeled. Default is <cite>auto</cite>, meaning that the sampler,
will be the default for the loss function used.
Setting to <cite>class</cite> will result in the <cite>ClassSampler</cite> being used, and setting to
<cite>random</cite> will result in the <cite>RandomSampler</cite> being used.
If set to <cite>random</cite> then <cite>num_items_per_class</cite> is not used.</p></li>
<li><p><strong>loss_optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the optimizer used for fine-tuning to loss function,
if it is a function that requires an optimizer. Options: <cite>Adadelta</cite>, <cite>Adagrad</cite>,
<cite>Adam</cite>, <cite>AdamW</cite>, <cite>SparseAdam</cite>, <cite>Adamax</cite>, <cite>ASGD</cite>, <cite>LBFGS</cite>, <cite>NAdam</cite>, <cite>RAdam</cite>,
<cite>RMSprop</cite>, <cite>Rprop</cite>, <cite>SGD</cite>. If left as None then optimizer specified by
the <cite>optimizer</cite> argument will be used instead.</p></li>
<li><p><strong>loss_optimizer_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – <p>Additional parameters to pass to the optimizer of
the loss function. The set of applicable parameters is specific to the optimizer
you choose. Details on the parameters can be found in the <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">PyTorch documentation</a>.</p>
</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless necessary, please stick with <cite>device=”cuda”</cite>, <cite>cpu</cite> training could be
extremely slow and inefficient.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.create_synthesis_run">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">create_synthesis_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corpus_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_relations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">run_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">description</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cuda'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">csv_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">public</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#finetuner.create_synthesis_run" title="Permalink to this definition">#</a></dt>
<dd><p>Create a Finetuner synthesis <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>, calling this function will submit a
data synthesis job to the Jina AI Cloud.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>query_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code> for example queries, name of a
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud, the dataset itself as
a list of strings or a path to a CSV file.</p></li>
<li><p><strong>corpus_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>]) – Either a <code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code> for corpus data, a name of a
<cite>DocumentArray</cite> that is pushed on Jina AI Cloud, the dataset itself as a
list of strings or a path to a CSV file.</p></li>
<li><p><strong>models</strong> (<a class="reference internal" href="../finetuner.model/#finetuner.model.SynthesisModels" title="finetuner.model.SynthesisModels"><code class="xref py py-class docutils literal notranslate"><span class="pre">SynthesisModels</span></code></a>) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">SynthesisModels</span></code> object containing the names of
the models used for relation mining and cross encoding.
You can pass <cite>finetuner.data.DATA_SYNTHESIS_EN</cite> for the recommended models for
synthesis based on english data.</p></li>
<li><p><strong>num_relations</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of relations to mine per query.</p></li>
<li><p><strong>run_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the run.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Name of the experiment.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>num_workers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of CPU workers. If <cite>cpu: False</cite> this is the number of
workers used by the dataloader.</p></li>
<li><p><strong>csv_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="../finetuner.data/#finetuner.data.CSVOptions" title="finetuner.data.CSVOptions"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code></a>]) – A <code class="xref py py-class docutils literal notranslate"><span class="pre">CSVOptions</span></code> object containing options used for
reading in training and evaluation data from a CSV file, if they are
provided as such.</p></li>
<li><p><strong>public</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – A boolean value indicates if the artifact is public. It should be
set to <cite>True</cite> if you would like to share your synthesized data with others.</p></li>
</ul>
</dd>
<dt class="field-even">Param<span class="colon">:</span></dt>
<dd class="field-even"><p>description: Run Description.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless necessary, please stick with <cite>device=”cuda”</cite>, <cite>cpu</cite> training could be
extremely slow and inefficient.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.get_run">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">get_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">run_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#get_run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.get_run" title="Permalink to this definition">#</a></dt>
<dd><p>Get a <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> by its name and (optional) <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code> name.</p>
<p>If an experiment name is not specified, we’ll look for the run in the default
experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>run_name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional name of the <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.list_runs">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">list_runs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">page</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#list_runs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.list_runs" title="Permalink to this definition">#</a></dt>
<dd><p>List all created <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> inside a given <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p>
<dl class="simple">
<dt>If no <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code> is specified, list <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> for all available</dt><dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The name of the <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p></li>
<li><p><strong>page</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The page index.</p></li>
<li><p><strong>size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> to retrieve.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<a class="reference internal" href="../finetuner.run/#finetuner.run.Run" title="finetuner.run.Run"><code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>List of all <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code>.</p>
</dd>
</dl>
<dl class="simple">
<dt>..note:: <cite>page</cite> and <cite>size</cite> works together. For example, page 1 size 50 gives</dt><dd><p>the 50 runs in the first page. To get 50-100, set <cite>page</cite> as 2.</p>
</dd>
</dl>
<p>..note:: The maximum number for <cite>size</cite> per page is 100.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.delete_run">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">delete_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">run_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#delete_run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.delete_run" title="Permalink to this definition">#</a></dt>
<dd><dl class="simple">
<dt>Delete a <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> given a <cite>run_name</cite> and</dt><dd><p>optional <cite>experiment_name</cite>.</p>
</dd>
</dl>
<p>If an experiment name is not specified, we’ll look for the run in the default
experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>run_name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the run. View your runs with <cite>list_runs</cite>.</p></li>
<li><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional name of the experiment.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.delete_runs">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">delete_runs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">experiment_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#delete_runs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.delete_runs" title="Permalink to this definition">#</a></dt>
<dd><p>Delete all <code class="xref py py-class docutils literal notranslate"><span class="pre">Run</span></code> given an optional <cite>experiment_name</cite>.</p>
<p>If an experiment name is not specified, we’ll delete every run across all
experiments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>experiment_name</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional name of the experiment.
View your experiment names with <cite>list_experiments()</cite>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.create_experiment">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">create_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#create_experiment"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.create_experiment" title="Permalink to this definition">#</a></dt>
<dd><p>Create an <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The name of the experiment. If not provided,
the experiment is named as <cite>default</cite>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment" title="finetuner.experiment.Experiment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code></a></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An <cite>Experiment</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.get_experiment">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">get_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#get_experiment"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.get_experiment" title="Permalink to this definition">#</a></dt>
<dd><p>Get an <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code> given a <cite>name</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the experiment.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment" title="finetuner.experiment.Experiment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code></a></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An <cite>Experiment</cite> object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.list_experiments">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">list_experiments</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">page</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#list_experiments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.list_experiments" title="Permalink to this definition">#</a></dt>
<dd><p>List all <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>page</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The page index.</p></li>
<li><p><strong>size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of experiments to retrieve.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment" title="finetuner.experiment.Experiment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list of <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code> instance.</p>
</dd>
</dl>
<dl class="simple">
<dt>..note:: <cite>page</cite> and <cite>size</cite> works together. For example, page 1 size 50 gives</dt><dd><p>the 50 experiments in the first page. To get 50-100, set <cite>page</cite> as 2.</p>
</dd>
</dl>
<p>..note:: The maximum number for <cite>size</cite> per page is 100.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.delete_experiment">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">delete_experiment</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#delete_experiment"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.delete_experiment" title="Permalink to this definition">#</a></dt>
<dd><p>Delete an <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code> given a <cite>name</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Name of the experiment.
View your experiment names with <cite>list_experiments()</cite>.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment" title="finetuner.experiment.Experiment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code></a></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Deleted experiment.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.delete_experiments">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">delete_experiments</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#delete_experiments"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.delete_experiments" title="Permalink to this definition">#</a></dt>
<dd><p>Delete all <code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code>.
:rtype: <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<a class="reference internal" href="../finetuner.experiment/#finetuner.experiment.Experiment" title="finetuner.experiment.Experiment"><code class="xref py py-class docutils literal notranslate"><span class="pre">Experiment</span></code></a>]
:return: List of deleted experiments.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.get_token">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">get_token</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#get_token"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.get_token" title="Permalink to this definition">#</a></dt>
<dd><p>Get user token from the Jina AI Cloud, <a class="reference internal" href="#finetuner.login" title="finetuner.login"><code class="xref py py-meth docutils literal notranslate"><span class="pre">login()</span></code></a> is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>user token as string object.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.build_model">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">build_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">select_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_onnx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#build_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.build_model" title="Permalink to this definition">#</a></dt>
<dd><p>Builds a pre-trained model given a <cite>name</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Refers to a pre-trained model, see
<a class="reference external" href="https://finetuner.jina.ai/walkthrough/choose-backbone/">https://finetuner.jina.ai/walkthrough/choose-backbone/</a>  or use the
<a class="reference internal" href="#finetuner.describe_models" title="finetuner.describe_models"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetuner.describe_models()</span></code></a> function for a list of all
supported models.</p></li>
<li><p><strong>model_options</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – A dictionary of model specific options.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Incoming documents are fed to the graph in batches, both to
speed-up inference and avoid memory errors. This argument controls the
number of documents that will be put in each batch.</p></li>
<li><p><strong>select_model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Finetuner run artifacts might contain multiple models. In
such cases you can select which model to deploy using this argument. For CLIP
fine-tuning, you can choose either <cite>clip-vision</cite> or <cite>clip-text</cite>.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>is_onnx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – The model output format, either <cite>onnx</cite> or <cite>pt</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>InferenceEngine</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>an instance of :class:’TorchInferenceEngine’ or
<code class="xref py py-class docutils literal notranslate"><span class="pre">ONNXINferenceEngine</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.get_model">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">artifact</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">select_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logging_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'WARNING'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_onnx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#get_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.get_model" title="Permalink to this definition">#</a></dt>
<dd><p>Re-build the model based on the model inference session with ONNX.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>artifact</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Specify a finetuner run artifact. Can be a path to a local
directory, a path to a local zip file, a Hubble artifact ID, or a huggingface
model created with finetuner. Individual model artifacts (model sub-folders
inside the run artifacts) can also be specified using this argument.</p></li>
<li><p><strong>token</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – A Jina authentication token (required for pulling artifacts from
Hubble) or a HuggingFace authentication token (required only when downloading
private models from huggingface). If not provided, the Hubble client might try
to find one either in a local cache folder or in the environment.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Incoming documents are fed to the graph in batches, both to
speed-up inference and avoid memory errors. This argument controls the
number of documents that will be put in each batch.</p></li>
<li><p><strong>select_model</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Finetuner run artifacts might contain multiple models. In
such cases you can select which model to deploy using this argument. For CLIP
fine-tuning, you can choose either <cite>clip-vision</cite> or <cite>clip-text</cite>.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use the CPU, if set to <cite>cuda</cite>, a Nvidia GPU will be used.
otherwise use <cite>cpu</cite> to run a cpu job.</p></li>
<li><p><strong>logging_level</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The executor logging level. See
<a class="reference external" href="https://docs.python.org/3/library/logging.html#logging-levels">https://docs.python.org/3/library/logging.html#logging-levels</a> for available
options.</p></li>
<li><p><strong>is_onnx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – The model output format, either <cite>onnx</cite> or <cite>pt</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>InferenceEngine</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">ONNXRuntimeInferenceEngine</span></code>.</p>
</dd>
</dl>
<dl class="simple">
<dt>..Note::</dt><dd><p>please install finetuner[full] to include all the dependencies.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="finetuner.encode">
<span class="sig-prename descclassname"><span class="pre">finetuner.</span></span><span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/finetuner/#encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#finetuner.encode" title="Permalink to this definition">#</a></dt>
<dd><dl class="simple">
<dt>Preprocess, collate and encode the <cite>list or :class:`DocumentArray</cite></dt><dd><p>with embeddings.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>InferenceEngine</em>) – The model to be used to encode <cite>DocumentArray</cite>. In this case
an instance of <cite>ONNXRuntimeInferenceEngine</cite> or <cite>TorchInferenceEngine</cite>
produced by <a class="reference internal" href="#finetuner.get_model" title="finetuner.get_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finetuner.get_model()</span></code></a></p></li>
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]) – The <cite>DocumentArray</cite> object to be encoded.</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Incoming documents are fed to the graph in batches, both to
speed-up inference and avoid memory errors. This argument controls the
number of documents that will be put in each batch.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DocumentArray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ForwardRef</span></code>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>DocumentArray</cite> filled with embeddings.</p>
</dd>
</dl>
<dl class="simple">
<dt>..Note::</dt><dd><p>please install “finetuner[full]” to include all the dependencies.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>

                </article>
            </div>
            <footer>
                
                <div class="related-pages">
                    
                    
                </div>
                <div class="bottom-of-page">
                    <div class="left-details">
                        <div class="copyright">
                            Copyright &#169; Jina AI Limited. All rights reserved.
                        </div><div class="last-updated">
                            Last updated on Jun 08, 2023</div>
                    </div>
                    <div class="right-details">
                        <div class="social-btns">
                            <a class='social-btn' href="https://github.com/jina-ai/finetuner/" aria-label="GitHub"
                               target="_blank" rel="noreferrer"> <i class="fab fa-github"></i></a>
                            <a class='social-btn' href="https://discord.jina.ai" aria-label="discord" target="_blank"
                               rel="noreferrer"> <i class="fab fa-discord"></i></a>
                            <a class='social-btn' href="https://youtube.com/c/jina-ai" aria-label="YouTube"
                               target="_blank" rel="noreferrer"> <i class="fab fa-youtube"></i></a>
                            <a class='social-btn' href="https://twitter.com/JinaAI_" aria-label="Twitter"
                               target="_blank" rel="noreferrer"> <i class="fab fa-twitter"></i></a>
                            <a class='social-btn' href="https://www.linkedin.com/company/jinaai/" aria-label="LinkedIn"
                               target="_blank" rel="noreferrer"> <i class="fab fa-linkedin"></i></a>
                        </div>
                    </div>
                </div>
                
            </footer>
        </div>
        <aside class="toc-drawer">
            
            
            <div class="toc-sticky toc-scroll">
                <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
                </div>
                <div class="toc-tree-container">
                    <div class="toc-tree">
                        <ul>
<li><a class="reference internal" href="#">finetuner package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-finetuner">Module contents</a><ul>
<li><a class="reference internal" href="#finetuner.login"><code class="docutils literal notranslate"><span class="pre">login()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.list_callbacks"><code class="docutils literal notranslate"><span class="pre">list_callbacks()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.list_models"><code class="docutils literal notranslate"><span class="pre">list_models()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.list_model_options"><code class="docutils literal notranslate"><span class="pre">list_model_options()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.describe_models"><code class="docutils literal notranslate"><span class="pre">describe_models()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.fit"><code class="docutils literal notranslate"><span class="pre">fit()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.synthesize"><code class="docutils literal notranslate"><span class="pre">synthesize()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.create_training_run"><code class="docutils literal notranslate"><span class="pre">create_training_run()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.create_run"><code class="docutils literal notranslate"><span class="pre">create_run()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.create_synthesis_run"><code class="docutils literal notranslate"><span class="pre">create_synthesis_run()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.get_run"><code class="docutils literal notranslate"><span class="pre">get_run()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.list_runs"><code class="docutils literal notranslate"><span class="pre">list_runs()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.delete_run"><code class="docutils literal notranslate"><span class="pre">delete_run()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.delete_runs"><code class="docutils literal notranslate"><span class="pre">delete_runs()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.create_experiment"><code class="docutils literal notranslate"><span class="pre">create_experiment()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.get_experiment"><code class="docutils literal notranslate"><span class="pre">get_experiment()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.list_experiments"><code class="docutils literal notranslate"><span class="pre">list_experiments()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.delete_experiment"><code class="docutils literal notranslate"><span class="pre">delete_experiment()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.delete_experiments"><code class="docutils literal notranslate"><span class="pre">delete_experiments()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.get_token"><code class="docutils literal notranslate"><span class="pre">get_token()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.build_model"><code class="docutils literal notranslate"><span class="pre">build_model()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.get_model"><code class="docutils literal notranslate"><span class="pre">get_model()</span></code></a></li>
<li><a class="reference internal" href="#finetuner.encode"><code class="docutils literal notranslate"><span class="pre">encode()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

                    </div>
                </div>
            </div>
            
            
        </aside>

        <qa-bot
            token="vizI4XaM1li8kQW2kQOiiBjtk02hzBDl0Em8lBjpzAKsjhX_z03mix_i3wKpiA=="
            theme="infer"
            target="_self"
            orientation="bottom-right"
            title="Finetuner"
            description="Task-oriented finetuning for better embeddings on neural search"
            show-tip>
          <template>
            <dl>
             <dt>You can ask questions about our docs. Try:</dt>
             <dd>What is Finetuner?</dd>
             <dd>How does Finetuner Work?</dd>
             <dd>What makes Finetuner unique?</dd>
            </dl>
          </template>
          <template slot="texts">
            <span for="tip">Hi there 👋
         Ask our docs!</span>
            <span for="unknownAnswerText">😵‍💫 I'm sorry but I don't know the answer.</span>
          </template>
        </qa-bot>
</div>
<img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=e0caedb8-a833-4e85-983d-d3394a5f16d2" /><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/furo.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vue@2/dist/vue.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/qabot"></script>
    <script src="../../_static/source-in-links.js"></script>
    </body>
</html>